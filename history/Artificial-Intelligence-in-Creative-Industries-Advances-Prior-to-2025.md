# Artificial Intelligence in Creative Industries Advances Prior to 2025

source: https://arxiv.org/html/2501.02725v1
Nantheera Anantrasirichai

## Contents
- [Abstract](#abstract)
- [Overview](#overview)
- [Key Technological Advancements](#key-technological-advancements)
- [AI Integration in Creative Processes](#ai-integration-in-creative-processes)
- [Challenges and Considerations](#challenges-and-considerations)
- [Emerging Trends](#emerging-trends)
- [Future Outlook](#future-outlook)
- [1 Introduction](#1-introduction)
- [2 Current Advanced AI Technologies](#2-current-advanced-ai-technologies)
  - [2.1 Transformers](#21-transformers)
  - [2.2 Large language models](#22-large-language-models)
  - [2.3 Diffusion Models](#23-diffusion-models)
  - [2.4 Implicit Neural Representations](#24-implicit-neural-representations)
- [3 Advanced AI for the creative industries](#3-advanced-ai-for-the-creative-industries)
  - [3.1 Content creation](#31-content-creation)
  - [3.2 Information analysis](#32-information-analysis)
  - [3.3 Content enhancement and post production workflows](#33-content-enhancement-and-post-production-workflows)
  - [3.4 Information Extraction and Understanding](#34-information-extraction-and-understanding)
  - [3.5 3D Reconstruction and Rendering](#35-3d-reconstruction-and-rendering)
  - [3.6 Data Compression](#36-data-compression)
  - [3.7 Visual Quality Assessment](#37-visual-quality-assessment)
- [4 Future Challenges for AI in the Creative Sector](#4-future-challenges-for-ai-in-the-creative-sector)
  - [4.1 Ethical Issues, Fakes and Bias](#41-ethical-issues-fakes-and-bias)
  - [4.2 The future of AI technologies](#42-the-future-of-ai-technologies)
- [5 Concluding Remarks](#5-concluding-remarks)

## Abstract

## Overview
-  Paper explores **AI advancements** since 2022, focusing on impact on **creative industries**
-  Highlights **technological shifts** in **generative AI** and **large language models (LLMs)**
-  Discusses expanded creative opportunities and enhanced efficiency

## Key Technological Advancements
-  **Text-to-image** technology improvements
-  **Text-to-video** capabilities enhancement
-  **Multimodal generation** technologies progress
-  **LLMs**: New benchmarks in **conversational AI**
-  **Image generators**: Revolutionary impact on content creation

## AI Integration in Creative Processes
-  **Post-production workflows**: Acceleration and refinement
-  **Content creation**: Democratization of access to creative tools

## Challenges and Considerations
-  **Media industry**: Increased demands on communication traffic
-  **Data compression**: Importance in managing creative content
-  **Quality assessment**: Crucial for maintaining standards

## Emerging Trends
-  **Unified AI frameworks**: Capable of addressing multiple creative tasks
-  **Human oversight**: Necessity to mitigate AI-generated inaccuracies

## Future Outlook
-  Exploration of **AI's potential** in the creative sector
-  Need to **navigate challenges** and **maximize benefits**
-  Importance of **addressing risks** associated with AI in creative industries

## 1 Introduction

**AI Advancements and Their Impact on Creative Industries**

**Generative AI and Large Language Models (LLMs):**
- Dramatic growth over past few years, with significant impacts on creative industries
- Focuses on generating new data that shares similarities to training data
- Output can serve as inspiration for individuals
- Opened up opportunities for users with different skill sets
- Major breakthrough: OpenAI's Generative Pre-trained Transformer (GPT) models
  - Designed to understand and generate human language
  - Large in terms of parameters and training data
  - Released ChatGPT, a conversational model with safety features
  - Other platforms: LaMDA, PaLM, Ernie Bot, BLOOM
  - Anthropic's Claude, trained specifically to be harmless and honest
- Quick and efficient responses, available for free

**Text-to-Image Models:**
- OpenAI's DALL·E 2: produces impressive artworks and photorealistic images
- Midjourney supports higher resolution images
- Stable Diffusion: publicly available code and model weights
- GPT-4: significantly larger model with improved performance, multimodal capabilities
  - Incorporates DALL·E 3 for broader range of nuances and details
- Claude 3 Opus: released by Anthropic, boasts multimodal capabilities in generating images, tables, graphs, diagrams

**Impact on Media Industry:**
- Concerns about the potential impact on print journalism due to AI tools
- Reduction of need for users to click through to news websites affects advertising and subscription revenues

**Generating Videos:**
- Significantly more challenging than generating images
- Google's Gemini 1.5: processes large data, including video footage and audio recordings
- OpenAI's Sora: generates impressive realistic videos up to 1 minute long (unreleased as of November 2024)
- Gemini 1.5 supports native audio understanding in 180+ countries

**Post-Production Workflow:**
- Generative AI may not have a direct impact on post-production workflow
- Neural networks originally proposed for generative AI widely adopted to serve this purpose, leading to significant improvements in output quality and computational speed

**Unified Framework:**
- Natural history filmmaking involves challenging acquisition environments and high production standards
- Painter by BAAI Vision: employs an image pair as a task prompt, enabling various tasks such as segmentation, low-light enhancement, rain removal, etc.

**Efficient Performance in Video Content Streaming:**
- Generative AI makes creating and post-processing digital content easier and faster
- Latest learning-based video codecs demonstrate potential to compete with conventional standard video codecs, emerging in the next few years

## 2 Current Advanced AI Technologies

**Foundation Models (FMs)**
- Terminology used by The Stanford Institute for Human-Centered Artificial Intelligence in 2021: "any model that is trained on broad data (generally using self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks"
- **LLMs**: One type of FM

**Introduction (from previous review)**
- AI introduction: neurons, CNNs, GANs, RNNs, DRL
- Emphasis on four rapidly growing technologies in creative industries around 2023: Transformers, LLMs, Diffusion Models (DMs), Implicit Neural Representations (INRs)
- Note that older technologies are still widely used, often in conjunction with newer ones for their distinct advantages.

**Transformers:**
- Part of FM technology gaining prominence
- Emphasizes **global dependencies**

**Large Language Models (LLMs)**
- Type of FM
- Part of rapidly growing technologies in creative industries around 2023

**Diffusion Models (DMs)**
- Part of the FM technology gaining prominence

**Implicit Neural Representations (INRs)**
- Part of the FM technology gaining prominence.

### 2.1 Transformers

**Advances in Artificial Intelligence: Transformers**

**ChatGPT**:
- Launched by OpenAI in November, 2022
- Fastest-growing consumer software application in history
- Significant advancement in performance of large language models (LLMs)

**Transformer Architectures**:
- Introduced by Google AI in 2017
- Dramatically improved the performance of language models
- Extended applications to vision understanding and learning multiple tasks across modalities

**Transformers vs. Recurrent Neural Networks (RNNs)**:
- Transformers capture long-range dependencies through self-attention mechanisms
- This enables parallel processing of the entire sequence, making transformers computationally efficient
- In contrast, RNNs process data sequentially

**Key Components of Transformer Architecture**:
- **Encoder**: Stack of identical layers with multi-head self-attention mechanism and feed-forward network
- **Decoder**: Stack of identical layers with additional sub-layer for multi-head attention over the output of the encoder stack

**Multi-Head Attention**:
- Set of several attention layers running in parallel
- Collectively focuses on information from distinct representation subspaces at various positions through learnable parameters

**Transformers in Computer Vision**:
- Integrated with CNNs for image classification, object detection, and other tasks
- First successful training of a transformer encoder for image recognition: Vision Transformer (ViT)
- Swin Transformer reported outperforming ViT by 2.4% in ImageNet-22K classification

**Transformers in Video Recognition**:
- Input as sequence of frames or divided into space-time blocks

**Advantages of Transformers**:
- Offer better performance across many tasks
- Open-source Transformers library through Hugging Face provides pretrained networks and datasets
- Emerging technologies have adopted state space models (Mamba), which are recognized as a linear variant of Transformers and offer similar or superior performance with fewer resources

### 2.2 Large language models

**Large Language Models (LLMs)**

**Overview**:
- Based on transformer models with self-attention mechanisms
- Two steps in training: pre-training and fine-tuning/prompt-tuning

**Pre-Training**:
- Unsupervised learning manner
- Learn meaning of words and relationships between them
- Examples: Generative Pre-trained Transformers (GPT)

**Fine-Tuning**:
- Training on new datasets for specific tasks
- Requires large enough data to generalize new task
- Prompt-tuning and prompt engineering for efficient use

**Prompt-Tuning**:
- Converts text inputs into task-specific virtual inputs (tokens)
- Trains a small set of parameters before utilizing LLM
- Relatively small amount of new data required

**Drawbacks**:
- Lack of interpretability

**Extensions**:
- Visual prompt tuning in other domains

**Resources**:
- Many LLM platforms (see Fig. 2a)
- Publications on surveys and evaluations, e.g., FLASK [^30]
- Evaluation of LLMs based on 12 fine-grained skills: logical correctness to harmlessness (Fig. 2b)

**Figure 2**:
- Timeline of large language models (a)
- Performance comparison evaluated by FLASK [^30] (b)

### 2.3 Diffusion Models

**Generative Models in Generative AI**
- **Autoencoders (AEs)**: Learn probability distribution of training data to generate new samples
  - Variational Autoencoders (VAEs): Improve results by learning latent space as statistical parameters
  - Compared to GANs: VAEs more stable during training, GANs better at realistic images

- **Generative Adversarial Networks (GANs)**: Two competing modules: generator and discriminator
  - Generator creates a sample, discriminator determines if real or generated
  - Compared to VAEs: Greater diversity in samples but less stable training process

- **Diffusion Probabilistic Models (DMs)**: Simplified process using denoising autoencoder for Bayesian inference
  - Denoising Diffusion Probabilistic Models (DDPMs): Denoise data by removing noise added at each iteration step
  - Score-based diffusion models: Use random noise to generate data with similar characteristics as training samples
    * Comparison to GANs: Provide higher diversity and more stable training but computationally intensive

- **Latent Diffusion Models (LDM)**: Convert images to feature maps, perform training in low-dimensional space
  - Conditional diffusion models: Generate synthesized samples based on provided information or "class"
    * Classifier guidance: Improve model's ability to generate desired class by providing more information
    * Transformers in LDM: Add cross-attention layer for flexible conditional image generators.

### 2.4 Implicit Neural Representations

**Implicit Neural Representations (INR)**
* Represents input content implicitly through learned functions F
* Can be seen as fields x parameterized by a neural network Φ, often an MLP
* Example: In image processing, INR inputs (x,y) to MLP and learns output (r,g,b)
* Weights and biases of MLP represent such an image
* Smaller number of parameters than total pixels multiplied by 3, offering data compression
* Handles complex and high-dimensional data efficiently in visual computing
* Traditional MLP uses ReLU for non-linear activation
* Sitzmann et al. demonstrate periodic functions like sinusoids are more suitable for natural signals
* Saragadam et al. propose complex Gabor wavelets to handle high frequencies and noise robustly
* Neural Radiance Fields (NeRF): a form of neural rendering that generates novel views based on 3D spatial coordinates and view directions, achieving realistic renderings with accurate lighting, shadows, and reflections.

**Implicit Neural Representations vs Explicit Neural Representations**
* INR methods are based on implicit neural representations
* Some compression and quality assessment tasks use dominant network architectures like LLMs or VQ-VAE models instead
* These methods are not covered in this paper.

## 3 Advanced AI for the creative industries

We categorize AI applications and solutions for the creative industries similar to our previous paper (Table [1](https://arxiv.org/html/2501.02725v1#S2.T1)). However, generative AI's effects vary by application, requiring different subsections.

### 3.1 Content creation

**AI Art and Content Generation**

**Background**
- Roots of AI art traceable to AARON program (1972) [^229]
- Practicality began with advancements in deep learning (GAN, transformers, DMs, INRs)
- Real camera content acquisition using AI: intelligent cinematography [^230]

**Text Generation**
- Assistance for various writing tasks [writing, blog posts, essays, reports]
- Beyond grammar and spelling checks
- Analyze style, tone, offer suggestions [image, videos, tables]
- Automate tasks like keywords, meta tags, SEO [^231]
- Transformers generate image captions [^44]
- Script generators aid writers, filmmakers, game developers [inspiration, idea generation]
- Longform creative writing still constrained for many tools [^232]
- Dramatron offers hierarchical language generation [Google] [^233]
- Chatbots generate content suitable for print or online publication [^234]

**Text Generation: Vision Language Models (VLMs)**
- Multimodal models that learn from images and text [Contrastive Language-Image Pre-training (CLIP)] [^236]
- Image encoder, embedding projector, text decoder [^237]
- Recent work scales up vision vocabulary [Scaling up CLIP model] [^45]

**Audio and Music Generation**
- Rapidly advanced due to unsupervised learning on large datasets and transformers [MuseNet, Magenta Studio, Musicfy] [10-12]
- Assist in composing music [learning complex musical patterns, predicting next word or note]
- Convert one type of sound into another [whistling to violin, flute to saxophone] [13]
- AI voice software changes voices and enables users to train models [Lalas, Kits, Media.io, Voice.ai] [16-19]

**Spatial Audio**
- Emerging technology in the field of spatial audio.

#### Advances in AI for Image and Video Generation (2023-24)

**Apple Music's Spatial Audio:**
- Over 80% of worldwide subscribers enjoying spatial audio experience as of 2022 (Apple Music)
- Head tracking enhances immersive experience
- Masterchannel launched SpatialAI, first spatial mastering AI
- Open-source tools built with Multimodal Diffusion Transformer architecture

**Image Generation:**
- Recent advances based on Diffusion Models (DMs)
- Competitive text-to-image models: Stable Diffusion, Midjourney, DALL·E, Ideogram
- Stable Diffusion 3 outperforms state-of-the-art systems in terms of typography and prompt adherence
- Integrates attention from both text and image

**Text-to-Image Generation:**
- Four models compared: Ideogram v1, DALL·E 3, Photoshop 2024, sdxy-turbo by Nvidia
- Difficulties in generating hands identified
- DALL·E 3 provides inpainting tool for user editing
- LLM-grounded Diffusion enables instruction-based scene specification with multiple rounds of user requests
- Generative Fill tool in Photoshop designed to generate new images or assist with photo editing
- InstructPix2Pix proposes a conditional diffusion model for image editing examples without predefined areas

**Video Generation and Animation:**
- Text-to-video generation starting to grow rapidly since 2024
- Major companies and private platforms releasing progress: Gemini 1.5 (Google), Make-A-Video (Meta), Sora (OpenAI)
- Make-A-Video uses spatiotemporally factorized diffusion model with joint text-image priors and super-resolution in space and time
- Gen-2 by Runway offers text- and image-to-video generation, generating a smooth 4-sec video
- Adobe Premier Pro integrates generative AI tools for video extension with third-party models.

#### AI in Creative Industries: Advances Prior to 2025

**AI Advances in Video Generation and Augmented Reality (AR/VR/MR)**
* **Text-to-video**: VASA-1 by Microsoft (2024), offers realistic facial expressions and head movements
* **Image-to-video**: Tune-A-Video, uses text prompt for style modification, leverages pretrained models
* **Text-guided video editing**: CVPR2023 competition highlighted temporal inconsistency issues in early methods
* **Transformer-based approaches**: CogVideo, Phenaki, generate variable length videos with lower quality than DM-based methods
* **New tools for video generation** (2024): Veo 2 by Google DeepMind, Kling AI, Pika 2.0, Hailuo AI
* **Text-to-3D post animation**: DeepMotion, offers text and video inputs
* **Augmented Reality (AR) and Mixed Reality (MR)**
	+ Rapid growth of AI-based 3D representation
	+ Real-time rendering with immersive interaction
	+ Merging real-world environments with computer-generated ones: Apple Vision Pro (2024)
* **AI tools for AR/VR/MR content generation**
	+ Holodeck: automatically generates 3D embodied environments via text prompt conversation
	+ RealFusion: merges 2D diffusion models with NeRF, improving Instant-NGP
	+ NeuralLift-360: uses diffusion models to generate prior for novel view synthesis
	+ Magic123: uses 2D and 3D priors simultaneously to produce high-quality assets.

### 3.2 Information analysis

**Text Categorization:**
- Applications: spam emails filtering, customer support automation, social media monitoring
- LLMs advantageous due to transfer learning capability
  - Pretrained on large text datasets
  - Fine-tuned on smaller task-specific datasets
- Examples of text categorization systems: CARP (kNN integration), ChatGraph (ChatGPT refinement)

**Advertisements and Film Analysis:**
- AI aids creators in matching content to audiences for personalized advertising
  - Nearly 9 out of 10 consumers comfortable with browsing history usage [^39]
- Advanced LLMs comprehend user intent behind searches, providing advertisers with granularity
- Sentiment analysis benefits from AI: opinion mining, film marketing campaigns, audience preferences analysis

**Content Retrieval and Recommendation Services:**
- Generative retrieval (GR) paradigm employs a single model to obtain results from query input
  - Advantages: reduced memory usage, enabling optimization during end-to-end training
- GR integrated with conversational question answering techniques for document retrieval
- Visual content retrieval enhanced by generative models decoding text queries into output samples
- DM employed for visual retrieval tasks, learning joint data distributions between text queries and video candidates
- Recommendation services operate based on previous usage patterns
  - Users define a specific query input vs. recommendation engines suggesting content

**Intelligent Assistants:**
- Software programs or applications that use AI and NLP to interact with users
- Ranges from simple chatbots to sophisticated virtual agents
- Designed to assist users in various tasks, such as answering questions, generating responses, controlling smart home devices
- LLMs enhance performance of intelligent assistants by understanding complex inquiries and generating natural conversational responses
  - Examples: Sasha, Google recommendation framework [^87]
- Generative AI can aid human customer support agents in search and summarization [^244]
- Potential applications for artists and customizing personal requirements.

### 3.3 Content enhancement and post production workflows

**AI Technologies for Enhancement: Contrast Enhancement, Colorization, Low-Light Environments, and Style Transfer**

**Contrast Enhancement:**
- Synthetic dataset introduced in LEDNet [^247] for low contrast scenarios
- Learnable non-linear activation function within the network to enhance feature intensities
- SNR-Aware [^92] estimates spatial-varying Signal-to-Noise Ratio (SNR) maps and proposes local and global learning branches
- NeRCo [^102] addresses low-light problem with INR, unifying diverse degradation factors
- Diffusion models (DMs) popular choices for low-light image enhancement: Diff-Retinex [^99], [^100], CLE Diffusion [^121]

**Colorization:**
- StyTr2 [^103]: first transformer-based method for style transfer, applying content as a query and style as a key of attention
- InST [^106]: utilizes Stable Diffusion Models and an attention-based textual inversion module to learn the description of the content
- StableVideo [^107]: uses a text prompt to describe the desired appearance based on diffusion model
- Large pre-trained DM employed for style transfer in [^105], where the style is injected to manipulate self-attention of decoder

**Upscaling Imagery: Super-Resolution (SR)**
- Transformers used extensively for single image super-resolution (SISR), outperforming CNNs in capturing long-term dependencies [^110]
- Hybrid Attention Transformer (HAT) [^112]: improves SR quality over ESRT by more than 2dB when upscaling 2 x -4 x
- Bicubic++ [^251]: fastest method in the NTIRE 2023 Real-Time Super-Resolution Challenge, using only convolutional layers and achieving 1.17ms in upscaling 720p to 4K images
- SR3 by Google [^116]: generates results based on statistics provided during training, not for restoration task but synthetic generation
- Trajectory-aware Transformer for Video Super-Resolution (TTVSR) [^111]: pioneering work incorporating transformers specifically for video SR tasks
- Adobe's VideoGigaGAN [^115]: achieves 8 x upsampling by adding flow estimation and temporal self-attention to the GigaGAN upsampler.

#### AI Applications in Image and Audio Restoration

**AI in Content Enhancement: Restoration and Upscaling**
* **Inverse problem approach**: Recent work uses a unified network architecture to address various distortions as inverse problems (y = hx + n), where x is the ideal data, y is the observed data, h is a degradation function (e.g., blur), and n is additive noise
* **Image restoration**:
  * SwinIR [^109]: Pioneering transformer-based method surpassing state-of-the-art CNN methods in super-resolution and denoising tasks
  * Uformer [^122], Restormer [^123]: Incorporate Transformer blocks into hierarchical encoder-decoder networks, focusing on restoring noisy images, sharpening blurry ones, and removing rain
* **Video restoration**:
  * Video Restoration Transformer (VRT) [^97], Recurrent Video Restoration Transformer (RVRT) [^93]: State-of-the-art methods for video super-resolution, deblurring, denoising, and frame interpolation
* **Audio restoration**:
  * Eliminating unwanted sounds: Well-established even before deep learning [^126]
  * Diffusion models: Proposed for audio bandwidth extension, inpainting, and declipping [^117]
* **Autofocus**:
  * AI-driven autofocus methods: Integrated into hardware, e.g., Choi et al.'s model for dual-pixel Canon cameras [^256], Yang et al.'s model utilizing semantic cues [^124]

#### Applications of AI in Image Processing and Visual Effects

**Advances in Image Manipulation using Transformers and CNNs**

**Dehazing**:
- Physics-inspired models outperform DehazeFormer by approximately 1dB

**Mitigating Atmospheric Turbulence**:
- Physics-inspired models have been developed to remove turbulence distortion
- Complex-valued CNNs exploit phase information
- Instance normalization (INR) offers tile and blur correction, but diffusion models outperform
- Transformer-based methods remain state-of-the-art for restoring videos

**Visual Inpainting**:
- CNNs and GANs have achieved impressive results in visual inpainting
- Recent work focuses on editing rather than just filling in missing areas
- Pluralistic inpainting allows users to generate multiple results to choose from
- Notable methods: Mask-Aware Transformer (MAT), PUT, Spa-former, DLFormer, ProPainter

**Image Fusion**:
- Involves merging multiple images into a single image
- Transformers and CNNs have been combined to extract global and local information
- Notable methods: SwinFusion, DDFM (first diffusion model-based image fusion)
- Diffusion models have been integrated into segmentation tasks to achieve superior results

**Editing and Visual Special Effects (VFX)**:
- Transformers and CNNs are used to edit specific areas of an image
- Tools like FacePoke, Motion-I2V allow users to modify images in real time
- Generative AI simplifies and accelerates automated processes in filmmaking and video production
- Limitations: Current technologies still cannot generate full 4K accurate visual effects

### 3.4 Information Extraction and Understanding

**Information Extraction and Understanding**

**Advancements in Information Extraction:**
- Automation and optimization through AI
- Recent advancements inspired by Language Models (LLMs)
- Utilization of prompts as conditional inputs
- Pipeline approach for pre-training models before fine-tuning

**Segmentation:**
- Central role in visual perception
- Integration of user-defined outputs: pixel-wise segmentation, bounding boxes, areas of interest
- Transformer architectures used: Segment Anything (SAM), HQ-SAM, SegGPT, SEEM, SAM2
- High-quality mask predictions with large datasets and global-local feature fusion
- Interactive segmentation interfaces
- Applications in semantic representation and medical domain
- Use of unsupervised domain adaptation (UDA) and INR for continuous rectification function modeling
- State-of-the-art panoptic segmentation based on text-to-image DMs

**Detection and Recognition:**
- Transformer architecture adoption: DETR, Swin Transformer, RT-DETR, YOLOv10
- Improvements in accuracy and training speed
- Integration of depth estimation module for 3D object detection: MonoDTR
- Applications of diffusion process denoising: DiffusionDet

**Tracking:**
- Benefits from transformers due to the need for attention in both space and time
- Top performer on various datasets: transformer-based methods.

### 3.5 3D Reconstruction and Rendering

**3D Reconstruction and Rendering**

**Integral to Creative Technologies:**
- Digital models for film, animation, games, VR/AR
- Seamless integration of live-action footage
- Dynamic environmental rendering
- Immersive experiences in VR/AR

**Depth Estimation:**
- Accurate depth information required for 3D models
- Depth sensors: lidar, structured light scanners
- Vision-based sensors: multiple cameras or single camera
- Monocular depth estimation from a single image
- Deep learning methods: transformers (e.g., [^179], [^180]), diffusion models (e.g., [^184], [^186])
- State-of-the-art method: Depth Anything v2 [^182]
  * Built on previous version
  * Large-scale labeled and unlabeled images
  * Semantic priors from pretrained encoders
  * Synthetic images and pseudo-labeled real images
  * Extracting features from DINOv2
  * Predicts depth of transparent and reflective surfaces

**Neural Radiance Fields (NeRF):**
- Learn a 3D scene from fewer images
- High-resolution photo-realistic novel views
- Flexibility on postprocessing
- Excels in neural rendering, view-dependent synthesis
- Challenges: accurately representing reflectance properties of the scene
- Camera positions estimated using feature mapping and SfM
- Each image mapped into camera rays generating 3D points with radiance
- Points processed by an MLP to predict volume density and emitted radiance
- Volume rendering techniques employed to generate an image for loss calculation
- Iteratively refines the model by minimizing this loss
- Variants: unbounded anti-aliased technique (Mip-NeRF360 [^183]), training from noisy RAW images (Google Research [^189]), integrations utilizing semantic segmentation (e.g., [^191])
- Reduced computational burden with Instant-NGP tool by Nvidia [^188] for real-time training
- Diffusion models integrated to regularize NeRF reconstructions ([^185])
- Leading methods: D-NeRF [^187], TiNeuVox [^190]
- Issues: large model size and/or long training time
- Compression methods: K-planes [^193], wavelet transform (e.g., [^192])

**Gaussian Splatting:**
- High-quality, unstructured representation of radiance fields
- Estimates sparse point cloud through SfM
- Each point possesses 3D Gaussian properties: position, covariance matrix, opacity, and spherical harmonics coefficients
- Optimization of parameters interleaved with steps controlling density of Gaussians

**Figure 7:**
- NeRFs (a), Gaussian Splatting (b), VR-GS system for 3D content interaction in VR (c)

### 3.6 Data Compression

**Image Compression:**
* Data compression essential for creative technologies to reduce memory space and bandwidth requirements
* Neural image codecs outperform standard image codecs since 2016: SwinT-ChARM [^199], STF [^200], LIC-TCM [^201]
* Transformer architectures used instead of CNNs for better compression efficiency
* Generative models, such as GANs and diffusion models, also employed for image compression: [^288][^202][^203][^204][^205]
* Notable works: COIN/COIN++ [^207][^209], DIRAC [^205], JPEG AI [^293]
* Public grand challenges held for fair evaluation and comparison of neural image codecs, such as Challenge on Learned Image Compression (CLIC) [^292]
* ISO/IEC developing a royalty-free learned image coding standard, JPEG AI [^294]

**Video Compression:**
* More challenging than image compression due to various immersive video formats and diverse content
* Predominant video coding standards: H.264/AVC, H.265/HEVC, H.266/VVC
* Learning-based video coding investigation in past five years
* Enhancement of conventional coding tools with deep learning techniques for improved performance [^296][^297][^298][^299][^300]
* Neural Network-based Video Coding (NNVC) [^301], achieving up to 13% coding gain over VTM 11 [^302] but requiring high computational complexity and GPU support
* Proposals for next generation video coding standard beyond AV1 focusing on trade-off between performance and complexity: AOM [^303]
* Research utilizing more advanced network architectures, such as ViTs [^304], diffusion models [^212] to further improve learning-based coding tools' performance.

### 3.7 Visual Quality Assessment

**Quality Assessment of Visual Signals**

**Assessing Visual Quality**:
- Objective models used instead of subjective tests for efficiency
- Used to evaluate performance of visual processing approaches
- Can be converted into loss functions for optimizing learning-based models
- Enhanced by deep learning techniques for better adaptation and performance

**Quality Assessment Models**:
1. **Full-Reference Models**: Require reference content for distorted test version
2. **No-Reference Models**: Do not require reference content
3. **Reduced-Reference Models**: Exist but less active in recent years
4. **Hand-Crafted Quality Metrics**: Based on features of human vision system
   - Structural similarity (SSIM, variants)
   - Distortion
   - Artifacts
5. **Regression-based Frameworks**: Combine hand-crafted features for more accurate prediction
6. **Learning-Based Quality Models**: Use neural networks and end-to-end optimization
   - Convolutional Neural Networks (CNNs): DeepQA, LPIPS, CONTRIQUE
   - Vision Transformers (ViTs) or similar variants: IQT, TRes, SaTQA, FastVQA, RankDVQA
7. **Multimodal Large Language Models (LLMs)**: Perform prompt-driven evaluation for image and video quality assessment
   - Q-Bench, Q-Align, X-iqe, 2AFC-LMMs, MAP estimation, ZEN-IQA, QA-CLIP, PromptIQA, BVQI, COVER

**Databases and Training Methodology**:
- Image/video databases with ground-truth subjective quality scores used for training and validation
  - LIVE, CSIQ, TID2013, PieAPP, PIPAL (images)
  - LIVE-VQA, KoNViD-1K, YouTube UGC, LIVE-VQC (videos)
- Databases for specific video formats and artifacts: LIVE-YT-HFR, VSR-QAD, BAND-2k, Maxwell/BVI-Artifact
- Training methodologies: Minimize difference between predicted quality indices and subjective scores
  - Limited number of ground-truth labels, expensive human testing
  - Use proxy quality metrics for weakly supervised training
    * RankIQA, UNIQUE (images)
    * VFIPS, RankDVQA (videos)
  - Self-supervised learning: CONTRIQUE, CONVIQT, quality-aware contrastive loss

## 4 Future Challenges for AI in the Creative Sector

**LLMs for Artistic Idea Expression:**
- **Top-down creative process**: Artists use text prompts to articulate their ideas, framing an overarching concept or goal before implementing details (contrasts with bottom-up approach)
- **Effectiveness of AI technologies**: Varies among artists based on their ability to provide accurate text inputs
- **Randomness in output generation**: The same text prompts may result in different outputs, making editing for precise thoughts challenging

**Coding and Debugging Tools:**
- Originally not designed for creative industries
- Help software developers expedite work within the sector by automating tasks and boosting productivity
- **Future of coding**: Nvidia CEO Jensen Huang believes traditional programming might become obsolete due to AI advancements, leading to automated tasks and increased efficiency for artists (not universally accepted)

**AI Coding for Creative Artists:**
- May need specific requirements to serve unstructural creativity
- **Artificial General Intelligence (AGI)**: Advanced form of AI that could exceed human intelligence and solve complex problems, but requires advanced computation, innovative algorithms, and understanding of the human brain.

### 4.1 Ethical Issues, Fakes and Bias

**AI Advancements and Ethical Concerns**
* **Advancements in AI**: Models like Sora and Gemini 1.5 Pro raise ethical concerns, including deepfakes and misinformation
* **Rapid AI development**: Raises questions about job displacement and the balance between automation and human involvement
* **Ensuring AI augments human efforts**: A significant challenge for developers and policymakers
* **Artistic demonstrations**: Highlight ethical dilemmas in AI, such as Miles Astray's "F L A M I N G O N E" exhibition
* **Uncanny valley effect**: Challenges perceptions of authenticity
* **Democratizing AI tools**: Opportunities but require robust regulatory frameworks to safeguard privacy

**Deepfake Technology**
* **Benefits and concerns**: Can generate realistic content, but also spread misinformation and have malicious uses
* **Detection and combat**: Developing algorithms, promoting media literacy, and implementing policies

**Unified Concept Editing**
* **Goals**: Ensure diverse representation, reduce biases, and produce safe content
* **Hallucinations in generative AI**: Generating outputs that are not faithful representations of reality

**AI and Copyrights**
* **Examples**: "Heart on My Sleeve" by an unidentified human author with AI-generated vocals, Grimes' invitation to emulate her voice for new musical pieces
* **Grimes' approach**: Advocates for the democratization of art and abolition of copyright, uses AI for visual content creation

### 4.2 The future of AI technologies

**Advancements and Challenges in AI**

**Dominant Companies in AI**: Google, Meta, NVIDIA, OpenAI, Anthropic
- Major players in data collection and sophisticated model training

**Performance Challenges**: Slow growth for OpenAI, Google, and Anthropic<sup class="ltx_note_mark">53</sup><sup class="ltx_note_mark">53</sup>
- Key issue: insufficient training data

**Applications of Language Models (LLMs)**:
- Complex tasks, advanced reasoning, data analysis, context understanding
- Require high computational resources or cloud computing for operation and fine-tuning

**Emergence of Small Language Models (SLMs)**:
- Microsoft's Phi-3<sup class="ltx_note_mark">54</sup><sup class="ltx_note_mark">54</sup>
- Promising solutions for regulated industries and sectors
- Can operate 'at the edge' on smartphones, etc. without relying on cloud connectivity

**Limitations of Current AI Models**:
- Struggle with planning ability and unexpected data
- Learn through reinforcement learning, but identify best outcome as anomaly

**Future Directions for AI**:
- **Self-supervised learning**: Yann LeCun<sup class="ltx_note_mark">55</sup><sup class="ltx_note_mark">55</sup>
  - Derive insights from unlabeled data
- **Iterative AI agentic workflows**: Andrew Ng<sup class="ltx_note_mark">56</sup><sup class="ltx_note_mark">56</sup>
  - Automate complex tasks with simple data entry and validation

**Openness of Code and Datasets**:
- Catalyst for accelerating AI advancements
- Necessitates new regulatory measures to monitor models, standardize documentation, and assess safety risks.

## 5 Concluding Remarks

Here is the passage condensed:

This paper reviews recent AI advancements and their applications in various fields. Generative AI has rapidly grown in the creative sector, improving content creation, analysis, enhancement, extraction, and compression. It reduces manual labor and time, enabling faster production cycles. As it advances, it promises to unlock even more sophisticated capabilities.

