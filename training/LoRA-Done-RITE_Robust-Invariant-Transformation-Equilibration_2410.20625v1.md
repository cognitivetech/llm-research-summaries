# # LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization

Authors: Jui-Nan Yen (UCLA), Si Si (Google), Zhao Meng (Google), Felix Yu (Google), Sai Surya Duvvuri (UT Austin), Inderjit S. Dhillon (Google), Cho-Jui Hsieh (Google & UCLA), Sanjiv Kumar (Google)

Email addresses: junianyen@cs.ucla.edu, sisidaisy@google.com, mengzhao@google.com, felixyu@google.com, saisurya@cs.utexas.edu, isd@google.com, chohsieh@cs.ucla.edu, sanjivk@google.com

(Source: https://arxiv.org/html/2410.20625v1)

## Contents
- [Abstract](#abstract)
- [1 Introduction](#1-introduction)
- [2 Transformation Invariance for LoRA Optimization](#2-transformation-invariance-for-lora-optimization)
  - [2.1 Definition of Transformation Invariance](#21-definition-of-transformation-invariance)
  - [2.2 Existing Optimizers are not Scalar Scale Invariant](#22-existing-optimizers-are-not-scalar-scale-invariant)
  - [2.3 Benefits of Transformation Invariance](#23-benefits-of-transformation-invariance)
- [3 Our Proposed Optimizer](#3-our-proposed-optimizer)
  - [3.1 Diagonal preconditioning is not enough for transformation invariance](#31-diagonal-preconditioning-is-not-enough-for-transformation-invariance)
  - [3.2 Achieving Transformation Invariance](#32-achieving-transformation-invariance)
  - [3.3 Theoretical Analysis](#33-theoretical-analysis)
- [4 Related Work](#4-related-work)
- [5 Experimental Results](#5-experimental-results)
- [6 Conclusions](#6-conclusions)
- [Appendix A Appendix](#appendix-a-appendix)
  - [A.1 Hyperparameters](#a1-hyperparameters)
  - [A.2 Dataset](#a2-dataset)
  - [A.3 Proof of Theorem¬†1](#a3-proof-of-theorem1)
  - [A.4 Proof of Theorem¬†2](#a4-proof-of-theorem2)
  - [A.5 Proof of Theorem¬†3](#a5-proof-of-theorem3)
  - [A.6 Gemma2 Experiment Result](#a6-gemma2-experiment-result)

## Abstract

**LoRA-RITE: A Novel Adaptive Matrix Preconditioning Method for LoRA Optimization**

**Background:**
- Low-rank adaption (LoRA) is a popular parameter-efficient finetuning method for large language models (LLMs)
- Current LoRA optimizers lack transformation invariance, leading to suboptimal solutions and inefficient learning

**Introducing LoRA-RITE:**
- Novel adaptive matrix preconditioning method for LoRA optimization
- Achieves transformation invariance while being computationally efficient

**Benefits of LoRA-RITE:**
- Theoretical analysis demonstrates the benefit of this method
- Improvements over existing optimizers on various LLM tasks
  - Gemma 2B, 7B, and mT5-XXL models
  - Consistent improvements across multiple benchmarks: Super-Natural Instructions, HellaSwag, ArcChallenge, GSM8K, OpenBookQA

**Results:**
- Replacing Adam with LoRA-RITE during LoRA fine-tuning of Gemma-2B yields:
  - **4.6% accuracy gain** on Super-Natural Instructions
  - **3.5% accuracy gain** across other four LLM benchmarks

## 1 Introduction

**LoRA (Low-Rank Adaptation)**
* Popular parameter-efficient method for fine-tuning Large Language Models (LLMs)
* Freezes pretrained weights, introduces low-rank matrices into each layer: WinR^mxn ‚Üí W + Z = A_1B_1^‚ä§ = A_2B_2^‚ä§
* Reduces memory requirements and mitigates overfitting in limited data settings
* Recent research explores improvements over classic LoRA algorithm

**Challenges with Standard Optimizers for LoRA:**
* Updates not transformation invariant: Z = A_1B_1^‚ä§ = A_2B_2^‚ä§, but optimizers may yield different updates based on factorization
* Violates mathematical consistency and leads to inefficiencies during training
* One LoRA factor often dominates optimization process while the other remains nearly fixed
* Recent approaches employ different learning rates for factors partially mitigate this issue

**Motivation for a More Principled Optimizer:**
* Prove diagonal preconditioners cannot achieve transformation invariance
* Use matrix preconditioners instead

**LoRA-RITE (Robust Invariant Transformation Equilibration):**
* Novel optimizer designed specifically for LoRA optimization
* Employs a transformation-invariant preconditioner on the low-rank side
* Achieves transformation invariance without significant overhead
* Maintains transformation invariance when incorporating first and second moments

**Contributions:**
* Propose LoRA-RITE, the first adaptive matrix preconditioning optimizer for LoRA that is transformation-invariant
* Provide a convergence analysis for the method
* Utilizes matrix preconditioners with little overhead compared to first-order optimizers when r (LoRA rank) is much smaller than m and n
* Significantly improves performance across multiple datasets and architectures:
  * GSM8K dataset with Gemma 7B IT model: LoRA-RITE achieves a 55.50% accuracy rate, outperforming Adam (48.37%) and Lamb (50.64%).

## 2 Transformation Invariance for LoRA Optimization

**Transformational Invariance in LoRA Training**
- Explanation of transformational invariance concept within LoRA training
- Demonstration that many existing optimizers fail to meet this property when used with LoRA
- This deficiency results in inefficient learning in practice

### 2.1 Definition of Transformation Invariance

**LoRA Optimization: Transformation Invariance**

**LoRA Factors and Equivalence**:
- LoRA adds a low-rank matrix Z=AB^‚ä§ to the original weight matrix W
- Many different LoRA factors (A_1,B_1),(A_2,B_2) can represent the same finetuned weight Z
- Different parameterizations can lead to different updates to Z
- This suggests a serious inconsistency and potential for suboptimal updates

**Definition of Transformation Invariance**:
- **Transformation Invariance**: An optimizer's updates (Œ¥A_1,Œ¥B_1) and (Œ¥A_2,Œ¥B_2) satisfy A_1(B_1+Œ¥B_1)^‚ä§ = A_2(B_2+Œ¥B_2)^‚ä§
- Ensures the optimizer produces the same update, Œ¥Z, to the fine-tuned weights for any equivalent LoRA factorizations

**Condition for Transformation Invariance**:
- Œ¥A_1B_1^‚ä§ = Œ¥A_2B_2^‚ä§, A_1Œ¥B_1^‚ä§ = A_2Œ¥B_2^‚ä§, Œ¥A_1Œ¥B_1^‚ä§ = Œ¥A_2Œ¥B_2^‚ä§

**Scalar Scale Invariance**:
- Weaker version of transformation invariance
- Requires updates remain equivalent when LoRA factors are scaled up or down by a constant factor

### 2.2 Existing Optimizers are not Scalar Scale Invariant

**Scalar Scale Invariance in LoRA Optimization**

**Gradient Descent**:
- Not scalar scale invariant for LoRA optimization
- First term has a scale difference of 1/s^2
- Second term has a scale difference of s^2
- Third term remains identical
- Gradients can be arbitrary large when s goes to 0 or infinity

**Adaptive Updates (e.g., Adam)**:
- Also not scalar scale invariant for LoRA optimization
- Discrepancy in scale of terms in condition ([4](https://arxiv.org/html/2410.20625v1#S2.E4))
- Failing to satisfy scalar scale invariance

**Existing Optimizers**:
- Gradient Descent, Adam, Adagrad [^4], RMSProp [^25], and Shampoo [^8] are not scalar scale invariant for LoRA optimization.

### 2.3 Benefits of Transformation Invariance

**Importance of Transformation Invariance**
- **Transformation invariance**: ensures equivalent weight updates for same parameters
- Demonstrates more efficient feature learning as network width grows

**Efficient Feature Learning**
- Describes asymptotic training behavior of LoRA
- Requires both Œ¥AB^‚ä§x and AŒ¥B^‚ä§x to be O(1) with respect to network width n

**Conventional Optimizers vs. Transformation-Invariant Optimizer**
- Conventional optimizers do not satisfy efficient feature learning (Figure 1 in [^9])
- Lacking scalar scale invariance leads to one LoRA factor updating improperly while the other remains unchanged

**Theorem 1**
- Transformation-invariant optimizer with same update rule for A and B guarantees efficient feature learning

**Proof of Theorem 1**
- Given in Appendix.

## 3 Our Proposed Optimizer

**Proposed Algorithm**:
- Achieves transformation invariance
- Significantly outperforms previous LoRA optimizers in various tasks

Note: This response is concise and summarizes the main points from the original passage while maintaining its meaning.

### 3.1 Diagonal preconditioning is not enough for transformation invariance

**Diagonal Preconditioning vs Transformation Invariance for LoRA Optimization**

**Existing Optimizers**:
- Diagonal preconditioning methods like Adam and Adagrad assume the preconditioner P is a diagonal matrix
- Matrix preconditioning methods such as Shampoo and CASPR use non-diagonal P

**LoRA Factors AinR^mxr and BinR^nxr**:
- A_2 = A_1 * R
- B_2 = B_1 * R^-‚ä§, where R is an invertible matrix

**Gradient Calculation**:
- ‚àáA_2 = ‚àáZB_2 = ‚àá(A_1 * R^-‚ä§)

**Diagonal Preconditioning**:
- vec(Œ¥A)^‚ä§ = Œ¥A and vec(‚àáA)^‚ä§ = ‚àáA
- Œ¥A = -‚àáAP^‚ä§Œ¥A, B^‚ä§ = -‚àáAP^‚ä§B^‚ä§

**Transformation Invariance**:
- If A_1 and A_2 are equivalent LoRA pairs with preconditioners P_1 and P_2:
  - Œ¥A_2 * B_2^‚ä§ = -‚àáA_2 * P_2^‚ä§ * B_2^‚ä§
  - Œ¥A_1 * B_1^‚ä§ = -‚àáA_1 * (R^-‚ä§ * P_2 * R^-1) * B_1^‚ä§
- For arbitrary R, there might not exist diagonal preconditioners P_1 and P_2 that satisfy this condition, leading to non-diagonal transformation invariance

**Conclusion**:
- Matrix preconditioning is necessary to achieve transformation invariance.

### 3.2 Achieving Transformation Invariance

**LoRA-RITE Algorithm: Transformation Invariance for LoRA Optimization**

**Background:**
- Recognize that LoRA factors A and B can be decomposed into orthogonal bases and magnitudes
- U_A, U_B obtained through polar decomposition
- Gradients depend on both basis and magnitude

**Transformation Invariance:**
- Introduce concept of "unmagnified gradients" (‚àáÃÖA, ‚àáÃÖB)
- Remain invariant to transformations of LoRA factors
- Used for adaptive matrix preconditioning in proposed optimization method

**Update Rule for A:**
1. Œ¥A_t=-‚àáÃÖA_t(VÃÖ_A_t+œÅ_A_tI)^-1/2
2. Split into two parts:
   a) Adaptive preconditioning mechanism using unmagnified gradients
   b) Magnitude adjustment for different LoRA pairs
3. Satisfies transformation invariance condition ([4](https://arxiv.org/html/2410.20625v1#S2.E4))

**Incorporating Second Moment:**
- Accumulate second moment: VÃÖ_A_t=P_A_tVÃÖ_A _t-1P_A_t^‚ä§+‚àáÃÖA_t^‚ä§‚àáÃÖA_t
- Account for varying basis at each step using projection matrix P_A_t
- Monitor loss of information with d_Œª()
- Compensate by adding œÅ_A_tI to preconditioner
- Final proposed update rule: SÃÖ_A_t=‚àáÃÖA_t(VÃÖ_ A_t+œÅ_A_tI)^-1/2

**Incorporating First Moment:**
- Maintain first moment using projection matrix MÃÖ_A_t
- Update rule: MÃÖ_A_t=Œ≤_1MÃÖ_A_t-1P _A_t^‚ä§+(1-Œ≤_1)SÃÖ_A_t
- Final proposed update rule: Œ¥A_t=-MÃÖ_A_tR_B^-‚ä§

**LoRA-RITE Algorithm:**
1. Initialize: unmagnified first and second moment (MÃÖ_A_0=0, VÃÖ_A_0=0)
2. For t=1 to T do:
   a. Compute gradient ‚àáA_t
   b. Compute QR decomposition of LoRA factor B_t
   c. Compute unmagnified gradient ‚àáÃÖA_t and P_A_t
   d. Update second moment: VÃÖ_A_t, œÅ_A_t, escaped mass
   e. Update preconditioned step: SÃÖ_A_t
   f. Update first moment: MÃÖ_A_t
   g. Update model parameters A_t+1 with Œ¥A_t
3. end for

### 3.3 Theoretical Analysis

**Online Convex Optimization Analysis:**
* In online optimization setting, a parameter **Œ∏\_t** is chosen iteratively based on convex decision set **ùí¶**.
* After each decision-making step **Œ∏\_t**, a convex loss function **f\_t** is revealed, potentially adversarially.
* Regret accumulated by the algorithm up to step T: **Regret\_T = ‚àë\_{t=1}^T f\_t(Œ∏\_t) - min\_{Œ∏‚ààùí¶} ‚àë\_{t=1}^T f\_t(Œ∏)**.

**Bounding First-Order Term:**
* Bound the first-order term: **‚àáŒ∏\_t^‚ä§(Œ∏\_t-Œ∏^*)**.
* Use convexity of **f\_t** to connect it to loss function.
* In LoRA case, loss functions are not convex with respect to **Œ∏**, so we directly bound the first-order term instead.

**Constraints on Frobenius Norm:**
* Assume constrains for fine-tuned weight Z of each layer: **A\_F ‚â§ D\_A, B\_F ‚â§ D\_B**.
* Gradient satisfies: **‚àáZ\_F ‚â§ G**.

**Convergence Analysis:**
* Analyze convergence in simplified scenario with omitted first moment and summed second moment.
* LoRA-Rite method yields the following result:
	+ Theorem 3: **LoRA-RITE satisfies**: **1/T ‚àë\_{t=1}^T 1/Œ∑‚àáŒ∏\_t^‚ä§(Œ∏\_t-Œ∏\_{t+1}) = O(GT^-1/2)**.
		- Suggests convergence to a stable solution or no change in function value.
* Introduce additional assumption: Assumption 1, constraining **Q\_A\_t** and **P\_A\_t**.
* Stronger convergence result (Theorem 4): **1/T ‚àë\_{t=1}^T‚àáŒ∏\_t^‚ä§(Œ∏\_t-Œ∏^*) = O(GD\_AD\_BT^-1/2)**.

**Comparison with One-sided Matrix Adagrad:**
* Regret bound of LoRA-RITE: **O(GD\_AD\_BT^-1/2)** vs. one-sided matrix Adagrad: **O(G(D\_A^2+D\_B^2)T^-1/2)**.
* Advantage in imbalanced cases: **D\_A^2 + D\_B^2 >= 2D\_AD\_B**.
	+ Previous work has shown LoRA factors often exhibit such imbalances.

## 4 Related Work

**Related Optimizers**
* **Adagrad**: Accumulated second moments used for scaling updates in each coordinate (diagonal preconditioner)
* **Adam, RMSProp**: Standard methods for deep neural network training using diagonal preconditioners; lack transformation invariance when applied to LoRA
* **Shampoo**: Higher-order preconditioners with ùí™(m^2+n^2) additional memory overhead and periodic computation of roots L and R with ùí™(m^3+n^3) computational cost
* **LARS, Lamb**: Lack transformation invariance but ensure scalar scale invariance

**LoRA Optimization Variants**
* **DyLoRA, IncreLoRA, AdaLoRA**: Dynamically adjusting LoRA rank during training
* **DoRA, DeepLoRA**: Enhancing LoRA performance through addition of scaling matrices
* **LoRA+**: Uses two different learning rates for LoRA weights; leads to an extra hyperparameter to be tuned in practice
* **Riemannian gradient descent**: Only method in literature that satisfies transformation invariance, but does not incorporate momentum and adaptivity (ScaledAdam)

**Table 1: Experimental Results on Super-Natural instruction dataset**
(Results not provided in text)

**Table 2: Experimental Results on LLM benchmarking datasets**
(Results not provided in text)

## 5 Experimental Results

**LoRA Optimizer Evaluation Results**

**Comparison of Optimizers:**
- **Adam**: Most widely used default optimizer for LoRA finetuning
- **LoRA+**: Adam with different learning rates for A and B (B 4x larger than A)
- **ScaledAdam**: Variant of Adam designed for LoRA optimization
- **Shampoo**: Adaptive matrix preconditioning method
- **Lamb**: Variant of Adam that normalizes updates based on parameter norms
- **LoRA-RITE**: Proposed optimizer, transformation invariant

**Evaluation Datasets:**
- SuperNatural Instructions dataset: Collection of diverse NLP tasks
- 4 standard LLM benchmarking datasets: HellaSwag, ArcChallenge, GSM8K, OpenBookQA

**Evaluation Metrics:**
- Exact match accuracy (classification)
- ROUGE-L score (generation)

**Results on SuperNatural Instructions Dataset:**
- LoRA-RITE demonstrates superior performance across both classification and generation tasks
- Improvements over Adam: 2.3% to 4.9% accuracy on classification tasks, significant improvements in global training setting
- Lamb performs well but has significant gap from LoRA-RITE
- Transformation invariance crucial for optimal performance

**Results on Other LLM Benchmarking Datasets:**
- LoRA-RITE achieves best performance on all datasets, Lamb usually second best

**Ablation Study:**
- Consistent performance across different LoRA ranks and model architectures (Gemma 2B, mT5-XXL)
- Successfully applied to encoder-decoder architecture (mT5-XXL)

**Training Speed Comparison:**
- Small overhead of LoRA-RITE compared to first-order methods
- Slightly slower than Adam on Gemma 2B, difference decreases with larger model size
- Shampoo slower due to recomputing preconditioner infrequently

## 6 Conclusions

**New Transformation-Invariant Optimization Algorithm for LoRAs**

Current LoRAs lack transformation invariance, causing disparate updates and hindering feature learning. This often leads to suboptimal solutions. We present a novel algorithm that maintains comparable time/memory overhead as Adam while being transformation-invariant. Our algorithm consistently delivers higher accuracy than existing LoRA optimizers across various datasets and models.

## Appendix A Appendix

### A.1 Hyperparameters

**Hyperparameter Settings (Table 5)**

Based on initial experiments, we set weight decay and dropout probability to zero as they do not improve baseline performance when given a non-zero value. This information can be found in Appendix A.1 of the paper "LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization."

### A.2 Dataset

**Table 6**: Summary Information of LLM Benchmarking Datasets (<https://arxiv.org/html/2410.20625v1#A1.T6>) shows the overview of these datasets. For evaluation purposes, we use the test set, since it is more extensive than the development set, for assessing ArcChallenge.

### A.3 Proof of [Theorem¬†1](https://arxiv.org/html/2410.20625v1#Thmtheorem1 "Theorem 1. ‚Ä£ 2.3 Benefits of Transformation Invariance ‚Ä£ 2 Transformation Invariance for LoRA Optimization ‚Ä£ LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization")

**Bulleted Notes (Concise Version)**

- Notation: A\_1 = Œ∏(n^a), B\_1 = Œ∏(n^b), ‚àáZ = Œ∏(n^c), Œ∑ = Œ∏(n^d), where n is the network width and Œ∑ represents the learning rate. Z = A\_1B\_1^T, from chain rule we know ‚àáA = ‚àáZB, ‚àáB = ‚àáZ^T\*A
- Update Rule: Updates expressed as Œ¥A\_1 = Œ∏(n^xa + yb + zc + d), Œ¥B\_1 = Œ∏(n^xb + ya + zc + d)
- Scalar Invariance: If the update rule is scalar scale invariant, then for any A\_2 = n^\_Œ¥A\_1, B\_2 = n^\_-Œ¥B\_1 we have Œ¥A\_1B\_1 = Œ¥A\_2B\_2, which implies xa - (y+1)Œ¥ = 0 => y = x-1
- Equality of Updates: Œ¥A\_1B\_1 = Œ∏(n^xa + (x-1)b + sc + d), A\_1Œ¥B\_1 = Œ∏(n^xb + (x-1)a + sc + d) => xŒ¥ - (x-1)Œ¥ = 0 for all Œ¥, thus x = y+1
- Efficient Feature Learning: By selecting a proper learning rate (Œ∑=Œ∏(n^d)), AŒ¥Bx = Œ¥AB x = Œ∏(1), where x is the input vector

### A.4 Proof of [Theorem¬†2](https://arxiv.org/html/2410.20625v1#Thmtheorem2 "Theorem 2. ‚Ä£ Incorporating first moment. ‚Ä£ 3.2 Achieving Transformation Invariance ‚Ä£ 3 Our Proposed Optimizer ‚Ä£ LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization")

**Consistency of Matrices in LoRA Optimization**

- In LoRA optimization, matrices `X_AinR^mxr` and `H_AinR^rxr` are defined as consistent if their transposes, `U_BU_B^‚ä§` and `U_BH_AU_B^‚ä§`, are identical across all LoRA pairs.
- Given that `U_BU_B^‚ä§` is the same for all pairs, `(‚àáÃÖA)^‚ä§‚àáÃÖAU _B^‚ä§ = U_BU_B^‚ä§‚àáZ^‚ä§ ‚àáZU_BU_B^‚ä§` implies that `(‚àáÃÖA)^‚ä§‚àáÃÖA` is consistent.
- If `bar{V}_{bm{A}}_t-1}` is consistent, then `{{bm{P}}_{bm{A}}_t}}}bar{{bm{V}}}_{bm{A}}_t-1}}{bm{P}}_{bm{A}}_t} }^^T}` is consistent as well. When `bar{{bm{V}}}_{bm{A}}_0}=bf{0}`, it implies that `bar{{bm{V}}}_{bm{A}}_t}` is consistent.
- Using the equation `U_B(VÃÖ_A_t+œÅ_A_tI)^ -1/2U_B^‚ä§=(U_BVÃÖ_A_t U_B^‚ä§+œÅ_A_tU_BU_B^ ‚ä§)^-1/2`, both `SÃÖ_A_t` and `MÃÖ_A_t` are consistent, thereby completing the proof.

### A.5 Proof of [Theorem¬†3](https://arxiv.org/html/2410.20625v1#S3.Ex27 "Theorem 3. ‚Ä£ 3.3 Theoretical Analysis ‚Ä£ 3 Our Proposed Optimizer ‚Ä£ LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization")

**Lemma 1 (Online Optimization)**
* For matrix XinR^mxr , HinR^rxr : X_H = Tr(XHX^‚ä§)^1/2
* Lemmas for online optimization: Lemma 1 (Lemma 5.13 [^10]) and Lemma 2 (Lemma 5.13, 5.14 [^10]).

**Preconditioner X_A_t**
* X_A_t = R_B_t^-1VÃÖ_A _t^-1/2R_B_t^-‚ä§
* Unmagnified preconditioner: XÃÖ_A_t = VÃÖ_A_t^-1/2
* Lemma 2 applied to A factor: ‚àë_t=1^Tvec(‚àáA_t)^‚ä§ vec(Œ¥A_t) ‚â§ Œ∑‚àë_t=1^T‚àáA_t_X_A_t ^2 = Œ∑‚àë_t=1^T‚àáÃÖA_t_XÃÖ_A _t^2 ‚â§ 2Œ∑*Tr(VÃÖ_A_T^1/2)

**Proof of Theorem 3**
* Bounding the third term: (A_t-A_*)R_B_t^‚ä§¬≤XÃÖ_A_t^-1 - Q_AXÃÖ_A_t-1^-1Q_A_t ‚â§ D_A¬≤D_B¬≤XÃÖ_A_t^-1 - P_AXÃÖ_A_t-1^-1P_A_t
* Inequality: XÃÖ_A_t^-1 ‚âΩ P_A_tXÃÖ_A_t-1^-1P_A_t^‚ä§
* Summing up the second and third term: (2Œ∑+1/Œ∑Œº D_A¬≤D_B¬≤)*Tr(VÃÖ_A_T^1/2)
* Choosing Œ∑ = (1/‚àö(2))Œº^1/2D_AD_B : O(D_AD_BGT^-1/2).

### A.6 Gemma2 Experiment Result

**Gemma2 Experimental Results** \
Conducted experiments on Gemma2-2B and presented results in Table 8 [1]. Proposed method shows a slight advantage over baselines, but performance difference between methods is low due to limited improvement from LoRA finetuning for Gemma2 on these tasks.

\*Reference: [1] A.6 Gemma2 Experiment Result ‚Ä£ Appendix A ‚Ä£ LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization

