## STEPTOOL: A STEP-GRAINED REINFORCEMENT LEARNING FRAMEWORK FOR TOOL LEARNING IN LLMS
Yuanqing Yu1, Zhefan Wang1, Weizhi Ma2∗
, Zhicheng Guo1, Jingtao Zhan1, Shuai Wang3,
Chuhan Wu3, Zhiqiang Guo1, Min Zhang1∗
1Department of Computer Science and Technology, Tsinghua University
2Institute for AI Industry Research, Tsinghua University
3Huawei Noah’s Ark Lab
{yyq23, wzf23}@mails.tsinghua.edu.cn
{mawz, z-m}@tsinghua.edu.cn
## ABSTRACT
Despite having powerful reasoning and inference capabilities, Large Language
Models (LLMs) still need external tools to acquire real-time information retrieval
or domain-specific expertise to solve complex tasks, which is referred to as tool
learning. Existing tool learning methods primarily rely on tuning with expert
trajectories, focusing on token-sequence learning from a linguistic perspective.
However, there are several challenges: 1) imitating static trajectories limits their
ability to generalize to new tasks. 2) even expert trajectories can be suboptimal, and
better solution paths may exist. In this work, we introduce StepTool, a novel step-
grained reinforcement learning framework to improve tool learning in LLMs. It
consists of two components: Step-grained Reward Shaping, which assigns rewards
at each tool interaction based on tool invocation success and its contribution to
the task, and Step-grained Optimization, which uses policy gradient methods to
optimize the model in a multi-step manner. Experimental results demonstrate that
StepTool significantly outperforms existing methods in multi-step, tool-based tasks,
providing a robust solution for complex task environments. Codes are available at
https://github.com/yuyq18/StepTool.
## 1 INTRODUCTION
Large Language Models (LLMs) have demonstrated remarkable abilities in reasoning and inference,
leading to impressive performance across a wide range of tasks (Brown et al., 2020; Zeng et al., 2022;
OpenAI, 2023). However, some complex tasks that require real-time information or domain-specific
knowledge often exceed the capacities of LLMs alone. In recent years, tool learning (Qin et al., 2024;
Patil et al., 2023; Qin et al., 2023) has emerged as a promising solution by augmenting LLMs with
external tools (APIs). As shown in Figure 1, LLMs can dynamically select, invoke, and interact
with tools to receive real-time responses. After multi-step interactions with external tools, LLMs can
effectively gather the necessary information to complete complex and challenging tasks.
To enhance the tool-learning capabilities of LLMs, most approaches rely on Supervised Fine-Tuning
(SFT) (Qin et al., 2023; Patil et al., 2023), in which LLMs are trained to imitate expert-generated
trajectories in a text generation manner. Each trajectory is a sequence composed of a user’s query,
multiple tool-callings and responses, illustrated in Figure 1. Despite its straightforward implemen-
tation, SFT encounters two key limitations in training LLMs for tool learning. Firstly, imitating
static pre-defined tool sequences limits the model’s ability to adapt to new tasks or environments.
Secondly, expert trajectories can successfully complete tasks but may not be the optimal sequence of
tool invocations. Blindly imitating these trajectories can lead to suboptimal task-solving performance.
In addition to SFT, we propose using Reinforcement Learning (RL) as another strategy for tool
learning, offering a more dynamic perspective by treating tool learning as a sequential decision-
making process. Under the RL perspective, each step of tool invocation is considered as an action that
leads to a state transition, and models are trained from the action-state transitions. Previous works
have explored applying RL to optimize LLMs in aligning with human preferences (RLHF) (Christiano
et al., 2017; Ouyang et al., 2022) or mathematical reasoning (Lightman et al., 2023; Wang et al.,
2023; Shao et al., 2024). Nevertheless, these methods are not well-suited for tool learning due to
several key challenges: 1) Tool learning involves multiple decision steps and real-time feedback from
external tools and environments. In contrast, RLHF is single-step based, and the steps in mathematical
reasoning tasks are generated by the LLM itself, without feedback from the environment. 2) The
reward of each step in tool learning is more complex, as it should consider not only the success of the
tool invocation but also its contribution to task completion.
To harness the potential of RL in tool learning with multi-step environment interactions and address
the limitations of existing methods, we propose StepTool, a novel step-grained reinforcement learning
framework for tool learning, which models tool learning as a sequential decision-making process
and treats each tool interaction as a critical decision point that directly impacts task completion, as
shown in Figure 1. Specifically, StepTool consists of two core components: Step-grained Reward
Shaping and Step-grained Optimization. For Step-grained Reward Shaping, we design rewards at
each step based on both the accuracy of tool invocation and the contribution to the overall task
completion, taking into account characteristics of intermediate actions in this scenario, i.e., well-
defined formats and explicit task objectives. These step-grained rewards offer richer signals for
tool learning, effectively guiding the model in decision-making. For Step-grained Optimization,
we propose a step-grained reinforcement-based optimization method based on the theory of policy
gradient (Williams, 1992; Sutton et al., 1999). This method ensures adaptability to dynamic, multi-
step interactions, addressing the limitations of single-step approaches like RLHF.
In summary, this work makes the following contributions:
• We identify the limitations of static supervised fine-tuning (SFT) and the unsuitability of classic
RLHF for tool learning, and introduce StepTool, a novel step-grained reinforcement learning
framework. StepTool considers tool learning as a multi-step decision-making process, enabling
models to learn from action-state transitions with real-time environment feedback.
• We design step-grained rewards tailored to tool learning scenarios, focusing on both the accuracy
of tool invocation and the contribution to the overall task. These richer signals guide the model’s
decision-making. Additionally, we propose a step-grained optimization method based on policy
gradients, ensuring adaptability to dynamic, multi-step interactions.
• Comprehensive experiments with three open-sourced models demonstrate the effectiveness of
StepTool, confirming its superiority in enhancing the performance of solving complex tasks.
## 2 RELATED WORK
### 2.1 TOOL LEARNING
Recent advancements in tool-augmented LLMs have expanded their ability to utilize external tools
for complex tasks. Early research (Chen et al., 2023; Shen et al., 2024; Schick et al., 2024) propose
to enable LLMs to interact with diverse external tools like program executors, search engines, and
QA system. Building on these initial efforts, subsequent models have focused on more extensive
interactions with real-world APIs and tools. citetqin2023toolllm, patil2023gorilla incorporate vast
APIs from platforms like RapidAPI and TorchHub, training LLaMA model (Touvron et al., 2023)
for tool-based tasks in a Supervised Fine-Tuning (SFT) manner. Additionally, some research efforts
have concentrated on constructing verifiable and diverse datasets for SFT training (Tang et al., 2023;
Abdelaziz et al., 2024; Liu et al., 2024).
Concurrent research (Chen et al., 2024) has explored the use of Direct Preference Optimization
(DPO) (Rafailov et al., 2024) for Tool Learning. However, this approach constructs preference data
pairs based on task completion, without accounting for the quality of intermediate steps. In contrast,
our work explicitly shapes step-grained rewards and leverages them for step-grained reinforced
optimization.
### 2.2 PROCESS SUPERVISION IN LLMS
Previous research (Lightman et al., 2023; Uesato et al., 2022; Ma et al., 2023; Shao et al., 2024;
Wang et al., 2023) have explored the effectiveness of process supervision in enhancing long-chain
reasoning abilities in LLMs. With a pre-trained process reward model, these methods optimize
reasoning using Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022). More
recent work (Lai et al., 2024) considers the correctness of each reasoning step and applies DPO
using step-level preference pairs. As noted earlier, our approach differs in two key aspects. First, in
mathematical reasoning, a "step" refers to text segments generated by LLMs, while in our work, steps
involve real-time interactions with external tools and environments. Second, mathematical rewards
focus on correctness relative to ground truth, whereas tool learning rewards account for both tool
invocation success and its contribution to task completion.
In the agent-based context, recent work (Xiong et al., 2024) has proposed step-level refinement to
enhance LLM agent performance, estimate step-level by Monte Carlo method and providing step-
by-step guidance. However, unlike their focus on exploration-based learning, our approach shapes
step-grained rewards tailored to tool learning scenarios and performs step-grained optimization using
policy gradient methods.
## 3 PROBLEM FORMULATION
In this work, we propose to model the tool learning process in LLMs as a multi-step decision-making
problem, which can be formulated as a Markov Decision Process (MDP). The MDP is represented by
the tuple M = (S, A, P, R, γ), with the following meanings:
• S: The state space, where each state st ∈ S represents the current context or environment responses
at time step t, in connection with prior tool interactions.
• A: The action space, where each action at ∈ A corresponds to calling an external tool (API) or
generating a final response (as a terminal action) at time t.
• P: The state transition dynamics, P (st+1|at, st) defines the probability of transitioning to a new
state st+1 given the current state st and the action at, representing how the environment changes
as tools are applied.
• R: The reward function, which assigns rewards rt = R(st, at) based on the current state st and
action at, representing the effectiveness of this tool-calling step.
• γ The discount factor, which determines how the model balances immediate rewards with long-term
task-solving performance.
Here we formulate the tool selection strategy of LLM as a decision-making policy πθ , parameter-
ized by θ, which governs the selection of actions (tools) based on the current state. A trajectory
τ = {s1, a1, s2, a2, ..., sT , aT } represents a sequence of states and actions over time, reflecting the
multiple interactions between LLMs and external tools or environments.
To maximize the final task-solving performance, the model seeks to optimize the expected reward
Rθ , which is given by:
Rθ = X
τ
R(τ )πθ (τ ) = Eτ ∼πθ (τ ) [R(τ )] , (1)
where R(τ ) represents the reward for a given trajectory τ , and πθ (τ ) defines the probability of
generating that trajectory under the policy πθ . The gradient of the expected reward can be computed
to update the model’s parameters, thereby enhancing the task-solving capabilities of the LLM:
∇Rθ = X
τ
R(τ )∇πθ (τ ) = X
τ
R(τ )πθ (τ )∇ log πθ (τ )
= Eτ ∼πθ (τ ) [R(τ )∇ log πθ (τ )]
= Eτ ∼πθ (τ ),(st,at)∼τ
"
R(τ )
TX
t=1
∇ log πθ (at|st)
.
(2)
For a detailed proof of this gradient formulation, refer to Appendix A.
To enhance learning efficiency and stabilize training, we replace R(τ ) with the advantage function
ˆA(st, at) as most policy-gradient-based RL algorithms (Williams, 1992; Schulman et al., 2017) did,
which compares the expected return of a given action to the average return for that state:
ˆA(st, at) = Gn
t − V (st) = rt + γrt+1 + γ2rt+2 + . . . + γT −trT − V (st), (3)
where Gn
t represents the estimated future reward, and V (st) is the value function, estimating the
expected return when starting from state st and following the current policy thereafter.
## 4 METHOD
Aimed at enhancing LLMs’ ability to use multiple tools for complex task solving, we propose a
novel step-grained reinforcement learning framework, StepTool, which is designed around the core
principles of the advantage function (Equation 3) and the policy gradient formulation (Equation 2).
As illustrated in Figure 2, StepTool consists of two primary components: Step-grained Reward
Shaping and Step-grained Optimization. Step-grained Reward Shaping assigns rewards at each tool
interaction step, evaluating both the accuracy of tool invocation and the contribution to the overall
task completion. Step-grained Optimization applies policy gradient methods to optimize the model
in a multi-step manner. Together, these components provide step-grained feedback and optimize
multi-step decisions, enhancing task-solving performance in complex environments.
### 4.1 STEP-GRAINED REWARD SHAPING
Step-grained Reward Shaping provides step-level reward signals for intermediate steps, effectively
guiding the model in decision-making. In tool learning scenarios, the steps of tool invocation are
characterized by well-defined formats and explicit task-oriented goals, naturally lending themselves
to easier step-grained reward shaping. These step-grained rewards offer explicit feedback for each
action, addressing the limitations of delayed rewards.
4.1.1 STEP-GRAINED REWARD DESIGN
Considering well-defined formats and explicit task objectives of intermediate tool-calling actions,
we have designed two key factors: the success of the tool call action (abbreviated as SuccCalling),
and the contribution to the overall task completion (abbreviated as Contribution). For the final step,
we directly link the reward to the completion of the task (abbreviated as IsSolved), representing to
whether the user’s query is solved.

Step-grained Optimization
Step 1 Step 2
token token token token
Step
QueryTool 1 Tool 2
Tool
Response Final
AnswerTool 1Tool 2
Tool 3Tool
Response Tool
ResponseTool
ResponseTool
ResponseTool
ResponseTool
ResponseTool
ResponseFinal
Answer
SuccCalling
Contribution
SuccCalling
Contribution
Annotators
Choices
Step 1 Step 2 Step
Policy Gradient
token token
IsSolved
Figure 2: The architecture of StepTool, a step-grained reinforcement learning framework, featuring
Step-grained Reward Shaping for assigning rewards at each tool interaction and Step-grained Opti-
mization for refining decision-making based on policy gradient.
SuccCalling. The SuccCalling metric evaluates whether the model successfully executes a tool call
with the correct format and content ( i.e. tool name and arguments). SuccCalling can be formally
represented as ˆrSC
t = SuccCalling(at, st+1), where the reward at time t is determined by the action
at and the subsequent state st+1.
However, simply making a correct tool call does not guarantee progress toward solving the task. To
further guide the model, we introduce the Contribution metric, which evaluates how much the tool’s
action aids the overall task solution.
Contribution. The Contribution metric evaluates the extent to which the tool’s action facilitates
the overall task solution. Actions that contribute minimally, such as redundant steps or irrelevant
outputs, receive lower rewards. The Contribution score is based on the relationship between the
current action and the final task-solving action, formally defined as ˆrCon
t = Contribution(at, aT ).
IsSolved. For the final step, the reward is directly associated with whether the task has been
successfully completed. The IsSolved metric evaluates the final answer based on the initial user query,
represented as ˆrIS
t = IsSolved(q, aT ). This reward only depends on the final step and the correctness
of the response in addressing the user’s query.
Formally, the reward for each action at step t is defined as:
ˆrt =
( α · ˆrSC
t + ˆrCon
t = α · SuccCalling(at, st+1) + Contribution(at, aT ), t = 1, 2, ..., T − 1
ˆrIS
t = IsSolved(q, aT ), t = T,
(4)
where α is a scaling factor to balance the weight of each component. To ensure consistency, rewards
for both the intermediate steps and the final step are normalized to a uniform scale.
4.1.2 STEP-GRAINED REWARD ACQUISITION
To generate training data with step-grained rewards, we first collect multiple trajectories from
the model’s own inferences across tasks in the training set, each comprising multiple interactions
between the model and external tools or environments. Step-grained rewards, derived from our
reward components, can be assigned through various methods, such as automated rule-based models,
human annotations, or advanced models like GPT-4 (OpenAI, 2023) (with the annotation prompts
detailed in Appendix B). Considering the significant time and financial costs associated with human
annotation, we primarily rely on a combination of rule-based systems and GPT-4 to handle the
annotation process. These step-grained annotated data can be used for offline reinforcement learning
optimization or to train a reward model for online training.
5
Published as a conference paper at ICLR 2025
### 4.2 STEP-GRAINED OPTIMIZATION
Addressing the limitations of single-step approaches like RLHF (Ouyang et al., 2022), we propose a
step-grained reinforced optimization strategy based on policy gradient that optimizes all prior steps,
ensuring adaptability to dynamic, multi-step interactions.
4.2.1 STEP-GRAINED OPTIMIZATION OBJECTIVE
Building on the problem formulation (Section 3), we now extend the gradient of the expected reward
to a token-level consideration. Assumed each action at consists of a sequence of Lt tokens, the
gradient of the expected return Rθ at the step level is expressed as:
∇Rθ = Eτ ∼πθ (τ ),(st,at)∼τ
" TX
t=1
ˆA(st, at)
LtX
i=1
∇ log πθ (ai
t|st, a1:i−1
t )
#
, (5)
where ˆA(st, at) represents the advantage function for the action sequence at at step t, which is
composed of Lt tokens. Through our step-grained reward shaping mechanism, we are able to
calculate rewards at each time step t in the trajectory. To better reflect the advantage of each action
sequence, we implement the advantage function ˆA(st, at) with our step-grained rewards ˆrt as:
ˆA(st, ˆrt, at) = Gn
t − V (st) = ˆrt + γ ˆrt+1 + γ2 ˆrt+2 + · · · + γT −t ˆrT − V (st). (6)
The term Gn
t reflects the cumulative future rewards based on these step-grained rewards ˆrt, discounted
by factor γ, extending from step t onward, while V (st) is the value function for the current state.
Our optimization objective is thus formalized as:
Lθ (π) = Eτ ∼πθ (τ ),(st,at)∼τ
" TX
t=1
ˆA(st, ˆrt, at)
LtX
i=1
log πθ (ai
t|st, a1:i−1
t )
#
. (7)
This objective reflects the optimization of the policy πθ by taking into account the step-level advantage
with our step-grained rewards, encouraging the model to select actions that yield higher reward gains.
Additionally, it should be noted that classic RLHF (Ouyang et al., 2022) typically optimizes “prompt-
response” data with final rewards based on human preferences, which is equivalent to treating the task
as a single step (T = 1). However, in the scenario of tool learning involving multi-step interactions
with external environments, each trajectory consists of multiple intermediate steps. Our method
addresses the more complex case of T > 1 by applying step-grained rewards and optimizing actions
at each step, ensuring both immediate and future outcomes are taken into account.
4.2.2 A PRACTICAL INSTANTIATION WITH PPO
Our framework is compatible with any policy gradient-based reinforcement learning algorithm. As a
practical example, we implement the Proximal Policy Optimization (PPO) (Schulman et al., 2017)
algorithm to demonstrate its application. Here, we estimate the advantage function using Generalized
Advantage Estimation (GAE) to improve stability:
ˆA(st, ˆrt, at) = δt + (γλ)δt+1 + · · · + (γλ)T −t+1δT −1,
δt = ˆrt + γV (st+1) − V (st). (8)
To achieve stable training, we employ the PPO-clip version, which introduces a clipping mechanism
to prevent large updates during optimization. The loss function based on the clipped PPO objective is
given by:
Lppo
θ (π) = ˆEτ ∼πθ (τ ),(st,at)∼τ
"
min
TX
t=1
ˆA(st, ˆrt, at)
LtX
i=1
log πθ (ai
t|st, a1:i−1
t )
log πθ′ (ai
t|st, a1:i−1
t ) ,
TX
t=1
ˆA(st, ˆrt, at)
LtX
i=1
clip
 log πθ (ai
t|st, a1:i−1
t )
log πθ′ (ai
t|st, a1:i−1
t ) , 1 − ϵ, 1 + ϵ
!#
,
(9)
6
Published as a conference paper at ICLR 2025
where πθ′ represents the represents the old policy used to generate the previous trajectories, and ϵ is a
hyperparameter that controls the allowable deviation between the current and old policies.
To further stabilize training, we also introduce a per-token KL divergence penalty from the old policy
at each token, as proposed in RLHF (Ouyang et al., 2022). This helps to prevent large policy shifts
during optimization. For our experiments, we apply the PPO version of our framework, which ensures
robust performance in multi-step tool-based tasks.
## 5 EXPERIMENTS
### 5.1 EXPERIMENTAL SETTINGS
Benchmark & Evaluation Metrics. ToolBench (Qin et al., 2023) is a widely recognized benchmark
in tool learning, evaluating model performance on solving complex tasks. In this work, we utilize
StableToolBench (Guo et al., 2024), an improved and more stable version of ToolBench. It features
765 solvable tasks across six subsets, with each subset varying in tool category and instruction
complexity, ranging from single-tool to multi-tool tasks. Detailed statistics are provided in Table 1.
We use two key metrics for evaluation: pass rate, measuring the proportion of tasks the model solves,
and win rate, indicating how often our method outperforms baselines. Additionally, we identify some
minor issues in the original ToolBench and StableToolBench codebases, and evaluate the baseline
after applying the necessary corrections to ensure consistency of evaluation.
Table 1: Statistics of test tasks in StableToolBench. Ins., Cat. and Tool stand for the Instruction,
Category, and Tool subgroup in the test set, respectively. The number of candidate APIs indicates the
total APIs considered during tool selection, while the number of relevant APIs reflects those that are
actually required to solve the given task.
I1 Ins. I1 Cat. I1 Tool I2 Cat. I2 Ins. I3 Ins.
# Tasks 163 153 158 106 124 61
# Candidate API 862 644 794 728 690 352
# Relevant API 371 328 358 301 261 180
Baselines. Due to the lack of publicly available implementation details for some concurrent works,
we choose two classic optimization methods: supervised fine-tuning (SFT) and PPO (Schulman et al.,
2017), as baselines. To validate the effectiveness of our proposed method, extensive experiments are
conducted on three open-source base models: ToolLlama-2-7b-v2(ToolLlama) (Qin et al., 2023),
Llama3.1-8B-Instruct (Llama3.1) (Touvron et al., 2023), and Qwen2-7B-Instruct (Qwen2) (Yang
et al., 2024). Each model adopts two different strategies: Chain of Thought (CoT) (Wei et al., 2022)
and Depth-First Search Decision Tree (DFSDT) (Qin et al., 2023), respectively. To ensure fairness in
data origins, we opt not to include Direct Preference Optimization (DPO) (Rafailov et al., 2024) in
our baselines, due to the requirement for constructing comparative data.
Training Setting. For SFT, Llama3.1 and Qwen2 are trained with static expert paths from GPT-
4 (OpenAI, 2023), with a training dataset sourced from ToolBench (Qin et al., 2023). ToolLlama is
directly applied as it had already been pre-trained in a similar manner. For PPO and our StepTool,
we obtain responses and interaction paths generated by each model towards user query samples of
5, 000 training tasks. We use both rule-based models and GPT-4 (gpt-4-turbo-2024-04-09)
to annotate step-grained. For a fair comparison, we optimize all models with the default learning rate
of 1e−5, batch size 8, and an initial KL coefficient 0.3 in the same experimental environment with
four NVIDIA A100 GPUs.
### 5.2 MAIN RESULTS
To demonstrate the effectiveness of our StepTool, we conducted performance comparisons using three
base models and two kinds of strategies. The comparative results are summarized in Table 2. For
reference, we also include the performance of the closed-source model gpt-3.5-turbo-0125
(GPT-3.5) as a benchmark. Below are some key observations:
7
Published as a conference paper at ICLR 2025
Table 2: Performance comparison between StepTool and other baselines. We run all models once and
take the average results from three times evaluations. StepTool performs best across all baselines
most of the time.
BaseModel Strategy Method Pass Rate (%)
I1 Ins. I1 Cat. I1 Tool I2 Cat. I2 Ins. I3 Ins. Average
GPT-3.5 CoT / 53.8±1.2 48.0±0.7 51.4±1.2 55.5±1.2 43.4±1.3 53.8±0.4 51.0±1.0
DFSDT / 60.0±0.5 53.5±1.3 65.7±0.5 61.6±1.2 50.5±0.7 65.6±2.7 59.5±1.2
ToolLlama
CoT
/ 54.2±0.5 50.3±0.8 56.5±1.5 52.0±0.6 45.4±0.6 37.2±1.0 49.3±0.8
PPO 55.0±1.9 50.5±0.9 42.3±0.7 46.4±0.7 42.1±1.6 35.2±1.2 45.3±1.2
StepTool 58.7±1.8 57.8±1.7 57.2±0.7 52.7±0.8 52.7±1.0 42.1±1.5 53.5±1.3
DFSDT
/ 57.0±1.0 52.3±1.5 57.5±1.2 52.4±0.7 49.7±1.7 53.8±1.9 53.8±1.3
PPO 57.5±1.5 54.2±0.5 53.5±2.0 50.8±1.2 48.1±0.8 43.2±0.4 51.2±1.1
StepTool 59.7±0.5 55.9±0.0 58.4±1.2 52.8±1.2 51.3±0.2 66.7±0.4 57.5±0.6
Llama3.1
CoT
SFT 53.9±1.2 52.6±1.4 51.9±0.9 52.2±1.7 44.7±0.4 36.3±0.8 48.6±1.1
PPO 50.2±0.9 57.8±0.8 53.0±0.6 52.3±1.6 49.2±1.5 38.0±1.5 50.1±1.2
StepTool 54.3±1.0 56.4±0.3 53.2±0.9 53.9±1.7 49.7±0.8 42.6±2.4 51.7±1.2
DFSDT
SFT 58.8±1.2 58.0±1.6 59.8±0.9 53.9±1.9 53.5±0.9 45.9±1.3 55.0±1.3
PPO 58.9±0.7 61.4±0.7 59.9±1.0 55.9±1.0 49.5±0.0 44.8±0.4 55.1±0.9
StepTool 59.3±0.8 60.9±1.3 60.2±1.3 56.2±1.6 59.3±1.4 50.5±1.0 57.7±1.2
Qwen2
CoT
SFT 53.0±0.6 54.5±0.7 59.9±1.2 54.0±0.3 45.6±1.4 40.7±0.8 51.3±0.8
PPO 58.8±0.9 54.9±0.7 57.0±0.5 54.3±1.0 45.1±1.0 48.4±3.1 53.1±1.2
StepTool 59.6±1.1 56.1±0.8 61.8±0.8 54.8±0.6 44.5±2.6 48.6±1.9 54.2±1.3
DFSDT
SFT 63.7±1.3 59.3±1.3 64.8±1.0 56.7±1.1 49.1±2.1 57.7±1.0 58.6±1.3
PPO 64.1±0.3 58.9±2.4 66.9±2.2 59.8±0.8 49.8±1.2 54.4±1.7 59.0±1.4
StepTool 65.6±1.8 60.8±0.3 68.4±1.6 60.9±0.9 51.1±1.8 65.3±1.7 62.0±1.4
• StepTool consistently outperforms SFT and PPO across most subsets for the same base model and
strategy, demonstrating the effectiveness of StepTool through step-grained optimization. Notably,
under the DFSDT strategy on the Qwen2 model, StepTool achieves a pass rate of over 60% on all
subsets except for ‘I2 Ins.’. This indicates the advantages of StepTool in highlighting its ability
to enhance task-solving performance, particularly when utilizing the DFS strategy in complex
scenarios.
• The performance improvements achieved by our method vary across different subsets. For instance,
on the ‘I1 Tool’ subset, StepTool outperforms the baseline by an average of 1%-4% in performance.
While, on the more complex subset ‘I3 Ins.,’ we observe substantial improvement ranging from 5%
to 13%. These results highlight the superior task-solving capabilities of StepTool, particularly in
tasks involving multiple tools across multiple categories.
• StepTool provides better solution paths compared to baselines, as evidenced by the win rate metric.
Using StableToolBench settings, we evaluated task-solving performance by comparing solution
paths from two models to determine which performed better. Figure 3 shows StepTool’s win rates0% 20% 40% 60% 80% 100%
Win Rate in I1 tool
StepTool v.s.
PPO (DFSDT)
StepTool v.s.
SFT (DFSDT)
StepTool v.s.
PPO (COT)
StepTool v.s.
SFT (COT)
50%
0% 20% 40% 60% 80% 100%
Win Rate in I2 Cat.
50%
Win Tie Lose
0% 20% 40% 60% 80% 100%
Win Rate in I3 Ins.
50%
Figure 3: Win rates of StepTool against other methods based on ToolLlama across three randomly
selected subsets. StepTool has a win rate over 50% against all baselines.
8
Published as a conference paper at ICLR 2025
against baselines across three subsets: ‘I1 Tool.’, I2 Cat.’, and ‘I3 Ins.’. StepTool consistently
outperformed both the SFT and PPO strategies on ToolLlama in CoT and DFSDT settings, with
win rates ranging from 50% to 65.8%, demonstrating its effectiveness in tool-based task solving.
### 5.3 ABLATION STUDY: IMPACT OF STEP-GRAINED COMPONENTS
We conduct an ablation study to evaluate the contribution of each step-grained component in StepTool.
We construct two variants: - w/o Step-grained Reward, where step-grained rewards are set to 0,
and - w/o Step-grained Opt, where the full trajectory is divided into sub-trajectories that end with
intermediate actions, and models are trained with PPO.
As Table 3 shown, removing the step-grained reward lowered the average pass rate to 48.1%,
while eliminating step-grained optimization reduced it further to 46.9%. This suggests that setting
intermediate rewards to zero fails to provide the necessary informative signals, leading to reduced
performance. Additionally, applying traditional PPO to sub-trajectories with step-level rewards
only optimizes steps in isolation, without leveraging the dependencies across multiple steps. These
findings highlight the importance of both components in our step-grained reinforcement learning
framework for solving complex multi-step tasks.
Table 3: Ablation study on two components of StepTool. Eliminating each component leads to
reduced performance.
Method Pass Rate (%)
I1 Ins. I1 Cat. I1 Tool I2 Cat. I2 Ins. I3 Ins. Average
ToolLlama + StepTool 58.7±1.8 57.8±1.7 57.2±0.7 52.7±0.8 52.7±1.0 42.1±1.5 53.5±1.3
- w/o Step-grained Reward 57.2±2.6 50.5±0.4 45.1±0.8 44.9±1.5 51.1±1.5 39.9±0.8 48.1±1.3
- w/o Step-grained Opt 57.7±1.5 52.2±1.3 43.0±1.4 45.3±0.8 41.8±1.1 41.5±1.5 46.9±1.3
ToolLlama 54.2±0.5 50.3±0.8 56.5±1.5 52.0±0.6 45.4±0.6 37.2±1.0 49.3±0.8
### 5.4 ANALYSIS OF TOOL INVOCATION SUCCESS RATESToolLLaMA Qwen2
78
79
80
81
82
83
84
85
86
Tool Invocation Success Rates (%)
80.18
82.73
82.37
82.9382.95
84.42
83.01
85.31Base Model (DFSDT)
+ StepTool (DFSDT)
Base Model (CoT)
+ StepTool (CoT)
Figure 4: Tool invocation success rates
for different methods using ToolLlama
and Qwen2 base models.
To verify the effectiveness of our method in improving
tool invocation during intermediate steps, we calculate
the average success rates of tool invocations across all
intermediate steps in the test sets for both ToolLlama and
Qwen2 models.
As illustrated in Figure 4, StepTool consistently improves
the success rates of intermediate tool invocations in both
CoT and DFSDT settings, demonstrating enhanced tool
accuracy and effectiveness in multi-step tasks.
### 5.5 QUALITATIVE ANALYSIS
We further perform the qualitative analysis to understand
how StepTool helps improve the intermediate actions. In Figure 5, we illustrate a case of correct-
ing wrong tool selection after applying our StepTool on ToolLlama. More cases can be found
in Appendix C.
In this example, the user requests channel information, video comments, and streaming sources for
movies. ToolLlama successfully retrieves channel info for "Paramount Pictures" and video comments
for ID "123456" but incorrectly calls the ‘getvideoscomment’ tool again instead of switching to the
‘download_stream’ tool for the third step. This error highlights issues in handling intermediate steps
before optimization. After applying our step-grained reinforcement strategy, the model correctly
uses the ‘download_stream’ tool, providing a streaming link and fulfilling the user’s request. This
demonstrates the importance of optimizing intermediate steps and the effectiveness of our approach
in improving decision-making in complex tasks.
9
Published as a conference paper at ICLR 2025
A Case of Correcting Wrong Tool Selection
ToolLlama
User Query: I’m planning a movie night
with my friends and I need some recom-
mendations. Get me the channel info for
’Paramount Pictures’, and the comments for
the video with ID ’123456’. Also, find a
website where I can stream or download the
movie with the ID ’UxxajLWwzqY’. Begin!
Step 1: Tool: getchannelinfo
Args: {"channel_id": "Paramount Pictures"}
Tool Response: Paramount Pictures is a ma-
jor American film studio, offering ...
Step 2: Tool: getvideoscomment
Args: {"video_id": "123456"}
Tool Response: {"user": "alicewonderland",
"comment": "Insightful video, learned a lot!",
"timestamp": "2023-10-05T08:21:45Z",
"likes": 230}, ...
Step 3: Tool: getvideoscomment
Args: {"video_id": "UxxajLWwzqY"}
Tool Response: {"user": "video_fanatic",
"comment": "This video blew my mind! The
quality is top-notch.", "timestamp": "2023-
10-09T14:00:00Z", "likes": 305}, ...
# Wrong Tool Selection
Step 4: Finish
...The channel info for ‘Paramount Pictures’...
The comments for the video with ID ‘123456’
are as follows: ‘Insightful video, learned a
lot!’ - alicewonderland, ... The comments
for the video with ID ’UxxajLWwzqY’ are as
follows: ‘This video blew my mind!...
Task Not Solved.
ToolLlama with StepTool
User Query: I’m planning a movie night
with my friends and I need some recom-
mendations. Get me the channel info for
’Paramount Pictures’, and the comments for
the video with ID ’123456’. Also, find a
website where I can stream or download the
movie with the ID ’UxxajLWwzqY’. Begin!
Step 1: Tool: getchannelinfo
Args: {"channel_id": "Paramount Pictures"}
Tool Response: Paramount Pictures is a ma-
jor American film studio, offering ...
Step 2: Tool: getvideoscomment
Args: {"video_id": "123456"}
Tool Response: {"user": "alicewonderland",
"comment": "Insightful video, learned a lot!",
"timestamp": "2023-10-05T08:21:45Z",
"likes": 230}, ...
Step 3: Tool: download_stream
Args: {"is_id": "UxxajLWwzqY"}
Tool Response: {‘id’: ‘UxxajLWwzqY’,
‘title’: ‘Icona Pop - I Love It (feat. Charli
XCX) [OFFICIAL VIDEO]’, ‘lengthSec-
onds’: ‘180’, STREAM ... DOWNLOAD ...
https://smarturl.it/UxxajLWwzqY ...}
Step 4: Finish
...The channel info for ‘Paramount Pic-
tures’...As for the video with ID ‘123456’,
the comments are as follows: ‘Insight-
ful video, learned a lot!’...Regarding
the movie with ID ‘UxxajLWwzqY’,
you can stream or download it from
https://smarturl.it/UxxajLWwzqY. Enjoy
your movie night!
Task Solved.
Figure 5: A case study on the I3 Ins. subset comparing ToolLlama and ToolLlama with StepTool:
StepTool successfully corrects the wrong tool selection.
6 CONCLUSION
In this work, we proposed StepTool, a novel step-grained reinforcement learning framework that
enhances LLMs’ ability to solve complex, multi-step tasks using multiple tools. StepTool features two
core components: Step-grained Reward Shaping, which provides feedback at each tool interaction by
evaluating tool invocation success and its contribution to the task, and Step-grained Optimization,
which uses policy gradient methods to optimize decision-making at each step. Experiments with
three open-sourced models demonstrate the effectiveness of StepTool, confirming its superiority in
enhancing the performance of solving complex tasks.
10
Published as a conference paper at ICLR 2025
For future work, while our reward mechanism based on pre-defined designs is simple but effective,
it remains a simplified version. Future work could focus on refining the reward design for greater
precision and expanding its applicability to a wider range of tasks.
LIMITATIONS
Despite the superior performance of this work, it is important to acknowledge several limitations.
First, as noted in prior studies (Shen et al., 2023; Rafailov et al., 2024), the training process with
PPO exhibits instability. We have provided all experimental setups and parameter settings in our
code repository for reference and reproducibility. Second, although our model demonstrates strong
performance, there remains potential for further improvement. Although our method supports training
a reward model for online multi-round data collection and optimization, we conducted only a single
round of offline training due to time and cost constraints. This may have limited the model’s potential
to achieve even greater performance gains.
REPRODUCIBILITY STATEMENT
To ensure reproducibility, we provide an anonymous GitHub repository containing all necessary
implementation code for our method, as well as the experimental setups, model configurations, and
scripts needed to reproduce our results. This repository can be accessed here: https://github.
com/yuyq18/StepTool.
REFERENCES
Ibrahim Abdelaziz, Kinjal Basu, Mayank Agarwal, Sadhana Kumaravel, Matthew Stallone, Rameswar
Panda, Yara Rizk, GP Bhargav, Maxwell Crouse, Chulaka Gunasekara, et al. Granite-function
calling model: Introducing function calling abilities via multi-task learning of granular tasks. arXiv
preprint arXiv:2407.00121, 2024.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
Sijia Chen, Yibo Wang, Yi-Feng Wu, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and
Lijun Zhang. Advancing tool-augmented large language models: Integrating insights from errors
in inference trees. arXiv preprint arXiv:2406.07115, 2024.
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting:
Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine
Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?
id=YfZ4ZPt8zd.
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences. Advances in neural information processing
systems, 30, 2017.
Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong
Sun, and Yang Liu. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of
large language models, 2024. URL https://arxiv.org/abs/2403.07714.
Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. Step-dpo: Step-
wise preference optimization for long-chain reasoning of llms. arXiv preprint arXiv:2406.18629,
2024.
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan
Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. arXiv preprint
arXiv:2305.20050, 2023.
11
Published as a conference paper at ICLR 2025
Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Shirley Kokane, Juntao Tan, Weiran
Yao, Zhiwei Liu, Yihao Feng, et al. Apigen: Automated pipeline for generating verifiable and
diverse function-calling datasets. arXiv preprint arXiv:2406.18518, 2024.
Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo Yuan, Pengfei Liu, Yang You, and Hongxia Yang.
Let’s reward step by step: Step-level reward model as the navigators for reasoning. arXiv preprint
arXiv:2310.10080, 2023.
OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow
instructions with human feedback. Advances in neural information processing systems, 35:27730–
27744, 2022.
Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model
connected with massive apis. arXiv preprint arXiv:2305.15334, 2023.
Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru
Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein,
Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master
16000+ real-world apis, 2023. URL https://arxiv.org/abs/2307.16789.
Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,
Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian,
Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei
Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang
Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu,
Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models, 2024. URL
https://arxiv.org/abs/2304.08354.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea
Finn. Direct preference optimization: Your language model is secretly a reward model. Advances
in Neural Information Processing Systems, 36, 2024.
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke
Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach
themselves to use tools. Advances in Neural Information Processing Systems, 36, 2024.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Y Wu,
and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language
models. arXiv:2402.03300, 2024.
Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan
Liu, and Deyi Xiong. Large language model alignment: A survey. arXiv preprint arXiv:2309.15025,
2023.
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:
Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information
Processing Systems, 36, 2024.
Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods
for reinforcement learning with function approximation. Advances in neural information processing
systems, 12, 1999.
Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. Toolal-
paca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint
arXiv:2306.05301, 2023.
12
Published as a conference paper at ICLR 2025
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia
Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and
outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.
Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Y Wu, and Zhifang
Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. CoRR,
abs/2312.08935, 2023.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in
neural information processing systems, 35:24824–24837, 2022.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8:229–256, 1992.
Weimin Xiong, Yifan Song, Xiutian Zhao, Wenhao Wu, Xun Wang, Ke Wang, Cheng Li, Wei Peng,
and Sujian Li. Watch every step! llm agent learning via iterative step-level process refinement.
arXiv preprint arXiv:2406.11176, 2024.
An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,
Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint
arXiv:2407.10671, 2024.
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint
arXiv:2210.02414, 2022.
13
Published as a conference paper at ICLR 2025
A PROOF OF EQUATION 2
The policy πθ (τ ), which describes the LLM’s decision-making process across the trajectory, can be
factored as:
πθ (τ ) = p(s1)πθ (a1|s1)p(s2|s1, a1)πθ (a2|s2)
= p(s1)
TY
t=1
πθ (at|st)p(st+1|st, at) (10)
Since the transition probabilities p(st+1|st, at) are governed by the environment and do not depend
on the policy, the gradient of log πθ (τ ) simplifies to:
∇ log πθ (τ ) =
TX
t=1
∇ log πθ (at|st) (11)
By substituting this expression into the gradient formula for ∇Rθ , we obtain the final form of the
policy gradient:
∇Rθ = Eτ ∼πθ (τ ) [R(τ )∇ log πθ (τ )]
= Eτ ∼πθ (τ )
"
R(τ )
TX
t=1
∇ log πθ (at|st)
# (12)
B REFERENCE PROMPT FOR STEP-GRAINED REWARD ANNOTATION
Here we provide a reference prompt for GPT-4 to perform step-grained reward annotation:
Instruction Prompt for Step-wise Reward
Query:
{query}
Intermediate Steps:
{mid_steps}
Final Answer:
{final_answer}
Given the above query, all intermediate steps and the final answer, you need to evaluate the
entire task-solving process by following rules:
(1) **Successful Tool Calling:** For each intermediate step, determine if a tool was called
successfully and give a score of 0 (no) or 1 (yes).
(2) **Contribution to Final Answer:** For each intermediate step, rate its contribution to the
final answer on a scale from 0 to 5.
(3) **Final Answer Status:** Determine if the final answer is “Solved”, “Unsure”, or
“Unsolved”.
Now provide your evaluation in JSON format with the parameters of “succeed_tool_calling”,
“contribution_to_final_answer” and “final_answer_status” to the function ‘evalu-
ate_process_reward”.
Figure 6: A Reference Prompt for Step-grained Reward Annotation.
14
Published as a conference paper at ICLR 2025
C MORE CASES
C.1 A CASE OF CORRECTING MISSING RELEVANT TOOL INVOCATION
This case from I2 Ins. illustrates that StepTool improves tool invocation by correcting missing relevant
tool invocation.
The user requests cocktail recipes with vodka and a cake recipe from the Birthday Cake DB. While
both systems successfully retrieve the cocktail recipe, ToolLlama fails to invoke the tool for the
detailed cake recipe, prematurely finishing the task. In contrast, ToolLlama with StepTool correctly
calls the tool, retrieving the complete cake recipe and solving the task. This demonstrates StepTool’s
effectiveness in enhancing tool invocation accuracy for multi-step tasks.
A Case of Correcting Missing Relevant Tool Invocation
ToolLlama
User Query: I’m hosting a cocktail party and
I need some cocktail recipes. Can you sug-
gest some cocktail recipes that contain vodka
as one of the ingredients? Also, provide me
with a cake recipe from The Birthday Cake
DB.Begin!
Step 1: Tool: search_by_ingredient_for_
cocktails Args: {"ingredient": "vodka"}
Tool Response: ...
Step 2: Tool: list_of_cakes_for_the_birthday
_cake_db Args: {}
Tool Response: {"id": "1", "title": "Rasp-
berry and custard muffins", "difficulty":
"Easy", "image": "https://apipics.s3..."},
{"id": "2","title":"Lemon and blackberry...} ...
# Miss A Relevant Tool Calling
Step 4: Finish
Sure! Here are some cocktail recipes that
contain vodka as one of the ingredients: 1.
Vodka Martini... lot!’...I can provide you with
a cake recipe from The Birthday Cake DB.
How about trying the Raspberry and custard
muffins? It’s an easy recipe and you can find
the image here: https://apipics.s3...
Task Not Solved.
ToolLlama with StepTool
User Query: I’m hosting a cocktail party and
I need some cocktail recipes. Can you sug-
gest some cocktail recipes that contain vodka
as one of the ingredients? Also, provide me
with a cake recipe from The Birthday Cake
DB.Begin!
Step 1: Tool: search_by_ingredient_for_
cocktails Args: {"ingredient": "vodka"}
Tool Response: ...
Step 2: Tool: list_of_cakes_for_the_birthday
_cake_db Args: {}
Tool Response: {"id": "1", "title": "Rasp-
berry and custard muffins", "difficulty":
"Easy", "image": "https://apipics.s3..."},
{"id":"2","title":"Lemon and blackberry...}
...
Step 3: Tool: detailed_cake_recipe_by_id
Args: {"is_id": "1"}
Tool Response: {‘ingredients’: ["140g un-
salted butter, melted", "3 medium free-range
eggs", "125g custard", ...], ‘method’: [{"Step
1": "Heat the oven to 170...}...]
Step 4: Finish
Sure! Here are some cocktail recipes that
contain vodka as one of the ingredients: 1.
Vodka Martini... lot!’... And here is a cake
recipe from The Birthday Cake DB: Rasp-
berry and custard muffins ... Ingredients: -
140g unsalted butter ...
Task Solved.
Figure 7: A case study on the I2 Ins. subset comparing ToolLlama and ToolLlama with StepTool:
ToolLlama missed a relevant tool invocation while StepTool successfully called the relevant tool and
completed the task.
15
Published as a conference paper at ICLR 2025
C.2 A CASE OF CORRECTING INCORRECT ARGUMENTS
This case from I2 Ins. illustrates that StepTool improves tool invocation by correcting incorrect
Arguments.
In this case, the user requests books by a specific author (J.K. Rowling) and interesting space news.
ToolLlama fails to retrieve the correct book information due to incorrectly formatted arguments passed
to the tool, resulting in incomplete search results. However, ToolLlama with StepTool successfully
corrects the tool arguments, retrieving the correct list of books along with the requested space news,
demonstrating StepTool’s ability to effectively handle incorrect arguments and complete the task.
A Case of Correcting Incorrect Arguments
ToolLlama
User Query: I’m a book blogger and I want
to write a post about books by a specific
author("J.K. Rowling"). Can you help me
search for books by the author’s name and
also provide me with interesting space news
to engage my readers? Begin!
Step 1: Tool: get _books_by_author_search
Args: {"authorization": "J.K. Rowling",
"page": "1", "search": ""}
Tool Response: Since the search input is
empty, please provide the name of an author
you are interested in ...
# Wrong Arguments for Tools
Step 2: Tool: get_space_news
Args: {}
Tool Response: {"title": "Launch Roundup:
SpaceX to launch Euclid; Virgin Galactic...",
"url": "https://www.nasaspace ...}
Step 3: Finish
I found some books by J.K. Rowling, but
the search results are incomplete. It seems
that the search function did not return any
books. However, I found some interesting
space news articles for you to engage your
readers: 1. ’Launch Roundup: SpaceX ...
Task Not Solved.
ToolLlama with StepTool
User Query: I’m a book blogger and I want
to write a post about books by a specific
author("J.K. Rowling"). Can you help me
search for books by the author’s name and
also provide me with interesting space news
to engage my readers? Begin!
Step 1: Tool: get _books_by_author_search
Args: {"authorization": "", "page": "1",
"search": "J.K. Rowling"}
Tool Response: {{"title": "Harry Potter and
the Sorcerer’s Stone", "publication_year":
1997, "genre": "Fantasy"}, {"title": "Harry
Potter and ...} ... }
Step 2: Tool: get_space_news
Args: {}
Tool Response: {"title": "Launch Roundup:
SpaceX to launch Euclid; Virgin Galactic...",
"url": "https://www.nasaspace ...}
Step 3: Finish
Here are some books by J.K. Rowling: 1.
Harry Potter and the Sorcerer’s Stone 2. Harry
Potter and ... And here are some interesting
space news articles: 1. Launch Roundup:
SpaceX to launch Euclid ...
Task Solved.
Figure 8: A case study on the I2 Ins. subset comparing ToolLlama and ToolLlama with StepTool:
StepTool successfully corrects incorrect arguments during tool invocation.
16